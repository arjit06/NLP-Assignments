{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "61e5c506-acb8-47ec-a580-e89877af4a3f",
    "_uuid": "99488814-35ef-4f57-8483-ea2f9c560954",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:16.572826Z",
     "iopub.status.busy": "2024-03-31T13:56:16.572142Z",
     "iopub.status.idle": "2024-03-31T13:56:18.296250Z",
     "shell.execute_reply": "2024-03-31T13:56:18.295169Z",
     "shell.execute_reply.started": "2024-03-31T13:56:16.572797Z"
    },
    "id": "JQg5EMQS401C",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = ('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PAD_ID = 0\n",
    "S_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "34b713c8-533a-4535-9a40-72f95c8b403c",
    "_uuid": "00f1922d-5124-4f41-9f4f-6aac8fc11bcb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:18.299379Z",
     "iopub.status.busy": "2024-03-31T13:56:18.298488Z",
     "iopub.status.idle": "2024-03-31T13:56:18.306028Z",
     "shell.execute_reply": "2024-03-31T13:56:18.305097Z",
     "shell.execute_reply.started": "2024-03-31T13:56:18.299343Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "d80a32d5-b36f-4e0b-bbf7-5d2db0f6725a",
    "_uuid": "aba97969-d955-4829-a3ad-1c6139a6847a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:18.307725Z",
     "iopub.status.busy": "2024-03-31T13:56:18.307412Z",
     "iopub.status.idle": "2024-03-31T13:56:18.314307Z",
     "shell.execute_reply": "2024-03-31T13:56:18.313431Z",
     "shell.execute_reply.started": "2024-03-31T13:56:18.307690Z"
    },
    "id": "xulyktDPUyiQ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "8a6ace8f-f28a-4ca4-9cd7-a0556771bbfb",
    "_uuid": "24ca354e-9b11-4999-bb50-df5b2529fc22",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:18.316627Z",
     "iopub.status.busy": "2024-03-31T13:56:18.316331Z",
     "iopub.status.idle": "2024-03-31T13:56:30.692794Z",
     "shell.execute_reply": "2024-03-31T13:56:30.691830Z",
     "shell.execute_reply.started": "2024-03-31T13:56:18.316604Z"
    },
    "id": "c_OCXnjuU7JG",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "3a5af79f-81b2-4ccd-f8e8-59c5885f6678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /raid/home/akshat21515/.local/lib/python3.8/site-packages (2.14.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: multiprocess in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: packaging in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pandas in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: aiohttp in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets) (2.22.0)\n",
      "Requirement already satisfied: xxhash in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.23.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.3.1)\n",
      "Requirement already satisfied: filelock in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0; python_version < \"3.11\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "c7896ce4-22d8-478e-aecb-293f8b16e0d5",
    "_uuid": "fcac7a39-d355-4162-80fe-a02ce597f899",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:30.694533Z",
     "iopub.status.busy": "2024-03-31T13:56:30.694194Z",
     "iopub.status.idle": "2024-03-31T13:56:33.929557Z",
     "shell.execute_reply": "2024-03-31T13:56:33.928738Z",
     "shell.execute_reply.started": "2024-03-31T13:56:30.694504Z"
    },
    "id": "CtKKuqrf401E",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "cb85eff5-f7b0-4d2f-ad19-7ea13cbf35de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/raid/home/akshat21515/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "train_data = datasets.load_dataset('wmt16','de-en',split='train[:50000]')\n",
    "val_data = datasets.load_dataset('wmt16',\"de-en\", split=\"validation\")\n",
    "test_data = datasets.load_dataset('wmt16','de-en', split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "d95cfee2-1f0b-47b3-9a6f-b9bd4babfe9f",
    "_uuid": "56bf7068-ea44-43ad-946d-c7c1b848b085",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:33.931092Z",
     "iopub.status.busy": "2024-03-31T13:56:33.930674Z",
     "iopub.status.idle": "2024-03-31T13:56:33.937224Z",
     "shell.execute_reply": "2024-03-31T13:56:33.936329Z",
     "shell.execute_reply.started": "2024-03-31T13:56:33.931066Z"
    },
    "id": "DBMQwRkr401E",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "a57a36a1-14de-4a07-f3f0-421dc6f945e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "a01834ed-c157-45dd-b95e-9b4c8a89d558",
    "_uuid": "1f1cc002-9b26-427a-a175-c9bf4f36161c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:33.938802Z",
     "iopub.status.busy": "2024-03-31T13:56:33.938512Z",
     "iopub.status.idle": "2024-03-31T13:56:33.947728Z",
     "shell.execute_reply": "2024-03-31T13:56:33.946633Z",
     "shell.execute_reply.started": "2024-03-31T13:56:33.938780Z"
    },
    "id": "J9A-A5ol401F",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1d756745-d5f6-4ae1-8aea-fa542e8fc58e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2169, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "07b7dc64-37e2-4a97-adbb-b96faa237642",
    "_uuid": "922f7c93-df60-4e9c-83a0-d569447fa6d9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:33.949892Z",
     "iopub.status.busy": "2024-03-31T13:56:33.949503Z",
     "iopub.status.idle": "2024-03-31T13:56:33.956715Z",
     "shell.execute_reply": "2024-03-31T13:56:33.955881Z",
     "shell.execute_reply.started": "2024-03-31T13:56:33.949862Z"
    },
    "id": "M-RZKTgP401F",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "a47dd93c-ebf4-47e7-d8fc-2700dc9f07eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2999, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "28f0b123-ecdc-403a-8ad0-170418c1d470",
    "_uuid": "dcf83789-9ee9-4e9b-aa83-ee38ea5c4d04",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:33.958150Z",
     "iopub.status.busy": "2024-03-31T13:56:33.957824Z",
     "iopub.status.idle": "2024-03-31T13:56:33.967605Z",
     "shell.execute_reply": "2024-03-31T13:56:33.966721Z",
     "shell.execute_reply.started": "2024-03-31T13:56:33.958120Z"
    },
    "id": "2Y70zE3e401F",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "class PositionalEncodings(Module):\n",
    "    def __init__(self, d_model:int, max_len:int=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        pe = torch.zeros((max_len, d_model))\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x:Tensor):\n",
    "        return (self.pe[:x.size(1), :]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "49e96c6f-8d71-483d-b9df-2823e209414b",
    "_uuid": "f6d55efc-ff31-4aba-8085-e849e79a7b9b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:33.971836Z",
     "iopub.status.busy": "2024-03-31T13:56:33.971234Z",
     "iopub.status.idle": "2024-03-31T13:56:33.977513Z",
     "shell.execute_reply": "2024-03-31T13:56:33.976568Z",
     "shell.execute_reply.started": "2024-03-31T13:56:33.971812Z"
    },
    "id": "3gZ8fEcA401F",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding\n",
    "\n",
    "class TokenEmbedding(Module):\n",
    "    def __init__(self, vocab_size:int, d_model:int):\n",
    "        super().__init__()\n",
    "        self.embed = Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x:Tensor):\n",
    "        return self.embed(x.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "cf370a2b-0ae9-4b7d-9fd9-02b85a2a35fa",
    "_uuid": "dc4348c4-9b86-4d21-a34c-80c9b255e519",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:33.978965Z",
     "iopub.status.busy": "2024-03-31T13:56:33.978696Z",
     "iopub.status.idle": "2024-03-31T13:56:33.992437Z",
     "shell.execute_reply": "2024-03-31T13:56:33.991562Z",
     "shell.execute_reply.started": "2024-03-31T13:56:33.978942Z"
    },
    "id": "Iru3lQjm401G",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Transformer,Linear,Sequential,Dropout\n",
    "from typing import Any,List\n",
    "\n",
    "class TransformerTranslation(Module):\n",
    "    def __init__(self, src_vocab_size:int,\n",
    "                 tgt_vocab_size:int,\n",
    "                 emb_sizes:List[int],\n",
    "                 d_model:int,\n",
    "                 nhead:int,\n",
    "                 num_encoder_layers:int,\n",
    "                 num_decoder_layers:int,\n",
    "                 dim_feedforward:int,\n",
    "                 dropout:float=0.1,\n",
    "                 activation:Any='relu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = Transformer(d_model=d_model,\n",
    "                                        nhead=nhead,\n",
    "                                        num_encoder_layers=num_encoder_layers,\n",
    "                                        num_decoder_layers=num_decoder_layers,\n",
    "                                        dim_feedforward=dim_feedforward,\n",
    "                                        dropout=dropout,\n",
    "                                        activation=activation,\n",
    "                                        batch_first=True)\n",
    "        self.generator = Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            Linear(d_model,tgt_vocab_size),\n",
    "            nn.LogSoftmax(dim=-1))\n",
    "        src_emb_size,tgt_emb_size = emb_sizes\n",
    "        self.src_emb = TokenEmbedding(src_vocab_size,src_emb_size)\n",
    "        self.tgt_emb = TokenEmbedding(tgt_vocab_size,tgt_emb_size)\n",
    "        self.src_pos = PositionalEncodings(src_emb_size)\n",
    "        self.tgt_pos = PositionalEncodings(tgt_emb_size)\n",
    "\n",
    "    def forward(self,\n",
    "                src:Tensor,\n",
    "                tgt:Tensor,\n",
    "                src_mask:Tensor=None,\n",
    "                tgt_mask:Tensor=None,\n",
    "                src_padding_mask:Tensor=None,\n",
    "                tgt_padding_mask:Tensor=None,\n",
    "                memory_key_padding_mask:Tensor=None):\n",
    "        src_inp = self.src_emb(src) + self.src_pos(src)\n",
    "        tgt_inp = self.tgt_emb(tgt) + self.tgt_pos(tgt)\n",
    "        output = self.transformer(src=src_inp,\n",
    "                                  tgt=tgt_inp,\n",
    "                                  src_mask=src_mask,\n",
    "                                  tgt_mask=tgt_mask,\n",
    "                                  memory_mask=None,\n",
    "                                  src_key_padding_mask=src_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_padding_mask,\n",
    "                                  memory_key_padding_mask=memory_key_padding_mask\n",
    "                                )\n",
    "        return self.generator(output)\n",
    "\n",
    "    def encode(self, src:Tensor, src_mask:Tensor):\n",
    "        return self.transformer.encoder(self.src_emb(src) + self.src_pos(src), src_mask)\n",
    "\n",
    "    def decode(self, tgt:Tensor, memory:Tensor, tgt_mask:Tensor):\n",
    "        return self.transformer.decoder(self.tgt_emb(tgt) + self.tgt_pos(tgt), memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "8332e212-559d-4291-a5ac-2f608e86664b",
    "_uuid": "6514ef45-36f7-413c-9213-c88654714e01",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:33.994282Z",
     "iopub.status.busy": "2024-03-31T13:56:33.993681Z",
     "iopub.status.idle": "2024-03-31T13:56:34.004814Z",
     "shell.execute_reply": "2024-03-31T13:56:34.003984Z",
     "shell.execute_reply.started": "2024-03-31T13:56:33.994234Z"
    },
    "id": "c3P9Cseq401G",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_subsequent_mask(dim:int):\n",
    "    mask = (torch.triu(torch.ones((dim,dim), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "9dfb3bb4-4ef7-469e-8d76-0007889e9714",
    "_uuid": "156364bc-63fc-43df-8dd0-565cf5237e74",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:34.006808Z",
     "iopub.status.busy": "2024-03-31T13:56:34.005995Z",
     "iopub.status.idle": "2024-03-31T13:56:34.014362Z",
     "shell.execute_reply": "2024-03-31T13:56:34.013568Z",
     "shell.execute_reply.started": "2024-03-31T13:56:34.006776Z"
    },
    "id": "kGOhWJqB401G",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_mask(src:Tensor, tgt:Tensor):\n",
    "    src_seq_len = src.size(1)\n",
    "    tgt_seq_len = tgt.size(1)\n",
    "    tgt_mask = generate_subsequent_mask(tgt_seq_len)\n",
    "    mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE)\n",
    "    src_mask = mask.float().masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "    src_padding_mask = (src == PAD_ID)\n",
    "    tgt_padding_mask = (tgt == PAD_ID)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "ea97b98c-e9e0-4544-989c-d33760105a16",
    "_uuid": "ed9d1edb-3bde-423f-8bea-d083dab8c5fe",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:34.015750Z",
     "iopub.status.busy": "2024-03-31T13:56:34.015416Z",
     "iopub.status.idle": "2024-03-31T13:56:34.023659Z",
     "shell.execute_reply": "2024-03-31T13:56:34.022846Z",
     "shell.execute_reply.started": "2024-03-31T13:56:34.015727Z"
    },
    "id": "TDB0O07C401H",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_vocab(dataset,language):\n",
    "    all_tokens = (' '.join(sample[language] for sample in dataset['translation'])).split()\n",
    "    vocab = set(all_tokens)\n",
    "    return all_tokens,vocab,len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "85bdd6c1-7c48-4acf-a3a5-3e20ce087adf",
    "_uuid": "f8f337de-281e-45be-b3df-0c78c5ce59af",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:34.025064Z",
     "iopub.status.busy": "2024-03-31T13:56:34.024806Z",
     "iopub.status.idle": "2024-03-31T13:56:41.362767Z",
     "shell.execute_reply": "2024-03-31T13:56:41.361851Z",
     "shell.execute_reply.started": "2024-03-31T13:56:34.025043Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer,models\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "src_tokens,src_vocab,src_vocab_size = compute_vocab(train_data,'de')\n",
    "tgt_tokens,tgt_vocab,tgt_vocab_size = compute_vocab(train_data,'en')\n",
    "\n",
    "tokenizer_de = Tokenizer(models.BPE())\n",
    "tokenizer_en = Tokenizer(models.BPE())\n",
    "\n",
    "special_tokens = ['<pad>', '<s>', '</s>', '<unk>']\n",
    "trainer = BpeTrainer(special_tokens=special_tokens)\n",
    "\n",
    "tokenizer_de.train_from_iterator(src_tokens,trainer=trainer)\n",
    "tokenizer_en.train_from_iterator(tgt_tokens,trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "86b37342-abdc-48ce-9e24-9137ecd90721",
    "_uuid": "38ec4610-0e9c-4841-9ec8-f1dbe9b22eb0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:41.364182Z",
     "iopub.status.busy": "2024-03-31T13:56:41.363897Z",
     "iopub.status.idle": "2024-03-31T13:56:42.915443Z",
     "shell.execute_reply": "2024-03-31T13:56:42.914632Z",
     "shell.execute_reply.started": "2024-03-31T13:56:41.364159Z"
    },
    "id": "FkXqbjih401H",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "d_model = 128\n",
    "emb_sizes = [128,128]\n",
    "nhead = 16\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 256\n",
    "batch_size = 32\n",
    "dropout = 0.2\n",
    "activation = 'relu'\n",
    "learning_rate = 0.0001\n",
    "betas = (0.33, 0.81)\n",
    "eps = 1e-9\n",
    "\n",
    "translator = TransformerTranslation(src_vocab_size,\n",
    "                                    tgt_vocab_size,\n",
    "                                    emb_sizes,\n",
    "                                    d_model,\n",
    "                                    nhead,\n",
    "                                    num_encoder_layers,\n",
    "                                    num_decoder_layers,\n",
    "                                    dim_feedforward,\n",
    "                                    dropout,\n",
    "                                    activation)\n",
    "for p in translator.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = optim.Adam(translator.parameters(),lr=learning_rate,betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "e0282976-f627-4b71-afca-2e9ea7cbc35e",
    "_uuid": "d1dad4f5-b278-4f13-92a4-aa4972fa3b9a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:42.917383Z",
     "iopub.status.busy": "2024-03-31T13:56:42.916840Z",
     "iopub.status.idle": "2024-03-31T13:56:42.924401Z",
     "shell.execute_reply": "2024-03-31T13:56:42.923473Z",
     "shell.execute_reply.started": "2024-03-31T13:56:42.917346Z"
    },
    "id": "B_KBgYu_401H",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tensor_transform(tokenizer,text):\n",
    "    token_ids = tokenizer.encode(text).ids\n",
    "    return torch.cat([torch.tensor([S_ID]),torch.tensor(token_ids)])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for sample in batch:\n",
    "        src_batch.append(tensor_transform(tokenizer_de,sample['translation']['de']))\n",
    "        tgt_batch.append(tensor_transform(tokenizer_en,sample['translation']['en']))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_ID, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_ID, batch_first=True)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "68a24369-243a-433b-b482-95e9332385e6",
    "_uuid": "560f3858-f4f3-4eb2-bb6d-491ff2e782f9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:42.926003Z",
     "iopub.status.busy": "2024-03-31T13:56:42.925612Z",
     "iopub.status.idle": "2024-03-31T13:56:42.938611Z",
     "shell.execute_reply": "2024-03-31T13:56:42.937838Z",
     "shell.execute_reply.started": "2024-03-31T13:56:42.925900Z"
    },
    "id": "dXx7fTqO401I",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "def model_epoch(model, data_loader, loss_fn, optimizer, is_train:bool):\n",
    "    model.to(DEVICE)\n",
    "    model.train(is_train)\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in data_loader:\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = generate_mask(src, tgt_input)\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "            tgt_out = tgt[:, 1:]\n",
    "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "6d4d15db-815e-4509-9fb5-a0cfc99de2cb",
    "_uuid": "2ab95a6c-07bd-4f69-8cbc-abbb6f8acdf7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T13:56:42.951697Z",
     "iopub.status.busy": "2024-03-31T13:56:42.951327Z",
     "iopub.status.idle": "2024-03-31T15:08:06.628384Z",
     "shell.execute_reply": "2024-03-31T15:08:06.626693Z",
     "shell.execute_reply.started": "2024-03-31T13:56:42.951654Z"
    },
    "id": "7ApitovH401I",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "2c3bbfd4-ee6f-4f42-c08e-ee19656dcc73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/akshat21515/.local/lib/python3.8/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/raid/home/akshat21515/.local/lib/python3.8/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 4.938961004188864\n",
      "Epoch 1/15 | Val Loss: 3.6660573342267204\n",
      "----------------------------------------------\n",
      "Epoch 2/15 | Train Loss: 2.759452155287725\n",
      "Epoch 2/15 | Val Loss: 3.5527605800067676\n",
      "----------------------------------------------\n",
      "Epoch 3/15 | Train Loss: 2.6707135252821392\n",
      "Epoch 3/15 | Val Loss: 3.5182547954952015\n",
      "----------------------------------------------\n",
      "Epoch 4/15 | Train Loss: 2.6285146834449136\n",
      "Epoch 4/15 | Val Loss: 3.5148937228847954\n",
      "----------------------------------------------\n",
      "Epoch 5/15 | Train Loss: 2.6008892336565945\n",
      "Epoch 5/15 | Val Loss: 3.478986179127413\n",
      "----------------------------------------------\n",
      "Epoch 6/15 | Train Loss: 2.5710763398913032\n",
      "Epoch 6/15 | Val Loss: 3.483608422910466\n",
      "----------------------------------------------\n",
      "Epoch 7/15 | Train Loss: 2.531700572255172\n",
      "Epoch 7/15 | Val Loss: 3.4585789144039154\n",
      "----------------------------------------------\n",
      "Epoch 8/15 | Train Loss: 2.5050010583527333\n",
      "Epoch 8/15 | Val Loss: 3.449372445835787\n",
      "----------------------------------------------\n",
      "Epoch 9/15 | Train Loss: 2.4858308948161736\n",
      "Epoch 9/15 | Val Loss: 3.4398337620146133\n",
      "----------------------------------------------\n",
      "Epoch 10/15 | Train Loss: 2.463601729691372\n",
      "Epoch 10/15 | Val Loss: 3.4134807534077587\n",
      "----------------------------------------------\n",
      "Epoch 11/15 | Train Loss: 2.4606221260280083\n",
      "Epoch 11/15 | Val Loss: 3.405577869976268\n",
      "----------------------------------------------\n",
      "Epoch 12/15 | Train Loss: 2.452247994112343\n",
      "Epoch 12/15 | Val Loss: 3.3897370327921474\n",
      "----------------------------------------------\n",
      "Epoch 13/15 | Train Loss: 2.4394220551160077\n",
      "Epoch 13/15 | Val Loss: 3.3855304840733025\n",
      "----------------------------------------------\n",
      "Epoch 14/15 | Train Loss: 2.414178096668429\n",
      "Epoch 14/15 | Val Loss: 3.380937346640755\n",
      "----------------------------------------------\n",
      "Epoch 15/15 | Train Loss: 2.4092543360825465\n",
      "Epoch 15/15 | Val Loss: 3.3895316001246956\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "train_losses, val_losses = [], []\n",
    "min_val_loss = 1e10\n",
    "bad_epochs = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = model_epoch(translator, train_loader, loss_fn, optimizer, is_train=True)\n",
    "    val_loss = model_epoch(translator, val_loader, loss_fn, optimizer, is_train=False)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if (val_loss<min_val_loss):\n",
    "        min_val_loss = val_loss\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs+=1\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss}')\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} | Val Loss: {val_loss}')\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    if(bad_epochs==num_epochs//7):\n",
    "        print(\"Stopping early...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "92c5af0b-0b18-46f3-8357-29bbb83a9d74",
    "_uuid": "9900ec21-8da7-4336-879d-75129f3248ad",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T15:08:15.402780Z",
     "iopub.status.busy": "2024-03-31T15:08:15.402042Z",
     "iopub.status.idle": "2024-03-31T15:08:15.702788Z",
     "shell.execute_reply": "2024-03-31T15:08:15.701832Z",
     "shell.execute_reply.started": "2024-03-31T15:08:15.402747Z"
    },
    "id": "wU0RD3nSzVmn",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(train_losses, label=\"Train\")\n",
    "if(len(val_losses)>0):\n",
    "    plt.plot(val_losses, label=\"Val\")\n",
    "\n",
    "plt.xlabel('Epochs',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "0f445665-cdbd-4809-a000-a1af415e3d50",
    "_uuid": "08c94f73-37d8-4ae1-834a-834f0e75668c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T15:08:15.704713Z",
     "iopub.status.busy": "2024-03-31T15:08:15.704420Z",
     "iopub.status.idle": "2024-03-31T15:08:15.712971Z",
     "shell.execute_reply": "2024-03-31T15:08:15.712067Z",
     "shell.execute_reply.started": "2024-03-31T15:08:15.704690Z"
    },
    "id": "YQVw_3giaw6A",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model,src,src_mask,max_len=512,start_symbol=S_ID):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    memory = model.encode(src, src_mask)\n",
    "    memory = memory.to(DEVICE)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        tgt_mask = (generate_subsequent_mask(ys.size(1))).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word[-1].item()\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        if next_word == EOS_ID:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T15:10:02.459964Z",
     "iopub.status.busy": "2024-03-31T15:10:02.459553Z",
     "iopub.status.idle": "2024-03-31T15:10:02.467106Z",
     "shell.execute_reply": "2024-03-31T15:10:02.465946Z",
     "shell.execute_reply.started": "2024-03-31T15:10:02.459933Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate_2A(model:Module,tokenizer_de,tokenizer_en,src_sentence:str):\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    src = (tensor_transform(tokenizer_de,src_sentence)).view(1,-1)\n",
    "    num_tokens = src.size(1)\n",
    "    src_mask = (torch.zeros(num_tokens,num_tokens))\n",
    "    tgt = greedy_decode(model,src,src_mask,num_tokens+5,S_ID).flatten()\n",
    "    return tokenizer_en.decode(tgt.cpu().numpy(),skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "ff6c2e17-12d4-4830-aaea-f4828ea8b709",
    "_uuid": "ebf72dc5-efbf-4336-8cf3-a745cbc59e70",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T15:10:02.470874Z",
     "iopub.status.busy": "2024-03-31T15:10:02.470525Z",
     "iopub.status.idle": "2024-03-31T15:10:15.088522Z",
     "shell.execute_reply": "2024-03-31T15:10:15.087308Z",
     "shell.execute_reply.started": "2024-03-31T15:10:02.470844Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /raid/home/akshat21515/.local/lib/python3.8/site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from evaluate) (2.14.7)\n",
      "Requirement already satisfied: xxhash in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: pandas in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.23.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from evaluate) (4.66.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from evaluate) (0.22.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from evaluate) (2.22.0)\n",
      "Requirement already satisfied: responses<0.19 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: dill in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: packaging in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n",
      "Requirement already satisfied: aiohttp in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyarrow-hotfix in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.0.0->evaluate) (5.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: filelock in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from responses<0.19->evaluate) (2.2.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (19.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0; python_version < \"3.11\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets>=2.0.0->evaluate) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 13:51:20.233873: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 13:51:21.049515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "340ee3d8-9be9-4677-9211-01bfc44c0b3c",
    "_uuid": "06edbece-91fc-4ed8-b818-4165447faa82",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-31T15:10:15.091773Z",
     "iopub.status.busy": "2024-03-31T15:10:15.090909Z",
     "iopub.status.idle": "2024-03-31T15:22:34.430729Z",
     "shell.execute_reply": "2024-03-31T15:22:34.429882Z",
     "shell.execute_reply.started": "2024-03-31T15:10:15.091731Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# candidates = []\n",
    "# references = []\n",
    "\n",
    "# for sample in test_data['translation']:\n",
    "#     src,tgt = sample['de'],sample['en']\n",
    "#     translation = translate_2A(translator,tokenizer_de,tokenizer_en,src)\n",
    "#     candidates.append(translation)\n",
    "#     references.append(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T15:22:34.432307Z",
     "iopub.status.busy": "2024-03-31T15:22:34.431984Z",
     "iopub.status.idle": "2024-03-31T15:22:36.406609Z",
     "shell.execute_reply": "2024-03-31T15:22:36.405598Z",
     "shell.execute_reply.started": "2024-03-31T15:22:34.432280Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|| 5.94k/5.94k [00:00<00:00, 3.51MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 4.47MB/s]                   \n",
      "Downloading extra modules: 100%|| 3.34k/3.34k [00:00<00:00, 2.26MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.03766300527701717\n",
      "BLEU-2: 0.0012564991334488734\n",
      "BLEU-3: 6.717345305135411e-05\n",
      "BLEU-4: 0.0\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load('bleu')\n",
    "\n",
    "bleu = bleu_metric.compute(predictions=candidates,references=references)\n",
    "print(f\"BLEU-1: {bleu['precisions'][0]}\")\n",
    "print(f\"BLEU-2: {bleu['precisions'][1]}\")\n",
    "print(f\"BLEU-3: {bleu['precisions'][2]}\")\n",
    "print(f\"BLEU-4: {bleu['precisions'][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T15:40:30.814206Z",
     "iopub.status.busy": "2024-03-31T15:40:30.813801Z",
     "iopub.status.idle": "2024-03-31T15:40:33.013303Z",
     "shell.execute_reply": "2024-03-31T15:40:33.011633Z",
     "shell.execute_reply.started": "2024-03-31T15:40:30.814175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /raid/home/akshat21515/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /raid/home/akshat21515/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /raid/home/akshat21515/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meteor Score: 0.03380934961341056\n"
     ]
    }
   ],
   "source": [
    "meteor_metric = evaluate.load('meteor')\n",
    "\n",
    "meteor = meteor_metric.compute(predictions=candidates,references=references)\n",
    "print(f\"Meteor Score: {meteor['meteor']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert_score in /raid/home/akshat21515/.local/lib/python3.8/site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from bert_score) (24.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from bert_score) (2.0.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from bert_score) (4.66.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from bert_score) (4.34.0)\n",
      "Requirement already satisfied: matplotlib in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from bert_score) (3.7.5)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from bert_score) (2.22.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bert_score) (1.23.3)\n",
      "Requirement already satisfied: torch>=1.0.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from bert_score) (2.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from pandas>=1.0.1->bert_score) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from pandas>=1.0.1->bert_score) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers>=3.0.0->bert_score) (5.3.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from transformers>=3.0.0->bert_score) (2024.4.16)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from transformers>=3.0.0->bert_score) (0.17.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from transformers>=3.0.0->bert_score) (0.14.1)\n",
      "Requirement already satisfied: filelock in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from transformers>=3.0.0->bert_score) (3.13.3)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from matplotlib->bert_score) (6.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->bert_score) (2.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from matplotlib->bert_score) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from matplotlib->bert_score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert_score) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from matplotlib->bert_score) (4.51.0)\n",
      "Requirement already satisfied: networkx in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.12\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (8.9.2.26)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib->bert_score) (3.18.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /raid/home/akshat21515/.local/lib/python3.8/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T15:42:17.073494Z",
     "iopub.status.busy": "2024-03-31T15:42:17.072751Z",
     "iopub.status.idle": "2024-03-31T15:42:27.571187Z",
     "shell.execute_reply": "2024-03-31T15:42:27.569347Z",
     "shell.execute_reply.started": "2024-03-31T15:42:17.073462Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "To be able to use evaluate-metric/bertscore, you need to install the following dependencies['bert_score'] using 'pip install bert_score' for instance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bertscore_metric \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbertscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m bertscore \u001b[38;5;241m=\u001b[39m bertscore_metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mcandidates, references\u001b[38;5;241m=\u001b[39mreferences, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(bertscore[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(bertscore[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/evaluate/loading.py:748\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m--> 748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m import_main_class(evaluation_module\u001b[38;5;241m.\u001b[39mmodule_path)\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/evaluate/loading.py:680\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (\u001b[38;5;167;01mConnectionError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m)):\n\u001b[0;32m--> 680\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    682\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hugging Face Hub either.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/evaluate/loading.py:633\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeasurement\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubEvaluationModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluate-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/evaluate/loading.py:489\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    488\u001b[0m imports \u001b[38;5;241m=\u001b[39m get_imports(local_path)\n\u001b[0;32m--> 489\u001b[0m local_imports \u001b[38;5;241m=\u001b[39m \u001b[43m_download_additional_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_hub_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# copy the script and the files in an importable directory\u001b[39;00m\n\u001b[1;32m    496\u001b[0m dynamic_modules_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_modules_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_modules_path \u001b[38;5;28;01melse\u001b[39;00m init_dynamic_modules()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/evaluate/loading.py:265\u001b[0m, in \u001b[0;36m_download_additional_modules\u001b[0;34m(name, base_path, imports, download_config)\u001b[0m\n\u001b[1;32m    263\u001b[0m         needs_to_be_installed\u001b[38;5;241m.\u001b[39madd((library_import_name, library_import_path))\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_to_be_installed:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be able to use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, you need to install the following dependencies\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[lib_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlib_name,\u001b[38;5;250m \u001b[39mlib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mneeds_to_be_installed]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlib_name,\u001b[38;5;250m \u001b[39mlib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mneeds_to_be_installed])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for instance\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_imports\n",
      "\u001b[0;31mImportError\u001b[0m: To be able to use evaluate-metric/bertscore, you need to install the following dependencies['bert_score'] using 'pip install bert_score' for instance'"
     ]
    }
   ],
   "source": [
    "bertscore_metric = evaluate.load('bertscore')\n",
    "\n",
    "bertscore = bertscore_metric.compute(predictions=candidates, references=references, model_type=\"distilbert-base-uncased\")\n",
    "print(f\"BERTScore: {sum(bertscore['precision'])/len(bertscore['precision'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /raid/home/akshat21515/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /raid/home/akshat21515/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /raid/home/akshat21515/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "\n",
    "bleu_metric = evaluate.load('bleu')\n",
    "meteor_metric = evaluate.load('meteor')\n",
    "# bertscore_metric = evaluate.load('bertscore')\n",
    "\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_2b = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "def translate_2B(model_2b, tokenizer, text):\n",
    "    input_text = \"translate German to English: \" + text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model_2b.generate(input_ids)\n",
    "    german_translation = tokenizer.decode(outputs[0])\n",
    "\n",
    "    return german_translation\n",
    "\n",
    "# translations = []\n",
    "# references = []\n",
    "# for item in val_data:\n",
    "#     english_text = item['translation']['en']\n",
    "#     german_translation = translate_2B(model_2b, tokenizer, english_text)\n",
    "#     translations.append(german_translation)\n",
    "#     references.append(item['translation']['de'])\n",
    "#     if(len(translations) % 500 == 0):\n",
    "#       print(len(translations))\n",
    "\n",
    "# bleu = bleu_metric.compute(predictions=translations, references=references)\n",
    "# meteor = meteor_metric.compute(predictions=translations, references=references)\n",
    "# # bertscore = bertscore_metric.compute(predictions=translations, references=references, lang='en')\n",
    "\n",
    "# print(f\"Validation BLEU-1: {bleu['precisions'][0]}\")\n",
    "# print(f\"Validation BLEU-2: {bleu['precisions'][1]}\")\n",
    "# print(f\"Validation BLEU-3: {bleu['precisions'][2]}\")\n",
    "# print(f\"Validation BLEU-4: {bleu['precisions'][3]}\")\n",
    "# print(f\"Validation METEOR: {meteor['meteor']}\")\n",
    "# print(f\"Validation BERTScore: {bertscore['f1']}\")\n",
    "\n",
    "# translations = []\n",
    "# references = []\n",
    "# for item in test_data:\n",
    "#     english_text = item['translation']['en']\n",
    "#     german_translation = translate_2B(model_2b, tokenizer, english_text)\n",
    "#     translations.append(german_translation)\n",
    "#     references.append(item['translation']['de'])\n",
    "#     if(len(translations) % 50 == 0):\n",
    "#       print(len(translations))\n",
    "      \n",
    "# bleu = bleu_metric.compute(predictions=translations, references=references)\n",
    "# meteor = meteor_metric.compute(predictions=translations, references=references)\n",
    "# # bertscore = bertscore_metric.compute(predictions=translations, references=references, lang='en')\n",
    "\n",
    "# print(f\"Test BLEU-1: {bleu['precisions'][0]}\")\n",
    "# print(f\"Test BLEU-2: {bleu['precisions'][1]}\")\n",
    "# print(f\"Test BLEU-3: {bleu['precisions'][2]}\")\n",
    "# print(f\"Test BLEU-4: {bleu['precisions'][3]}\")\n",
    "# print(f\"Test METEOR: {meteor['meteor']}\")\n",
    "# print(f\"Test BERTScore: {bertscore['f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU-1: 0.45546615170046134\n",
      "Test BLEU-2: 0.268678622207953\n",
      "Test BLEU-3: 0.17319673947841505\n",
      "Test BLEU-4: 0.11587960008508828\n",
      "Test METEOR: 0.34849179722066537\n",
      "Test BERTScore: [0.863649845123291, 0.921355664730072, 0.9038230776786804, 0.8924747705459595, 0.9258562326431274, 0.8644993305206299, 0.8429644703865051, 0.8770416975021362, 0.8773070573806763, 0.8750308156013489, 0.8455091118812561, 0.8470861911773682, 0.8262258172035217, 0.8300788402557373, 0.7915123701095581, 0.871814489364624, 0.8867651224136353, 0.8347976207733154, 0.8287848234176636, 0.8852449059486389, 0.9002132415771484, 0.848119854927063, 0.8794133067131042, 0.8976000547409058, 0.8844802379608154, 0.8714832663536072, 0.8623412847518921, 0.9692016243934631, 0.8655409216880798, 0.8796126246452332, 0.9730011224746704, 0.8910902738571167, 0.7956358194351196, 0.9542884230613708, 0.9006202816963196, 0.9619002938270569, 0.8976296186447144, 0.8671607971191406, 0.919253945350647, 0.9048522710800171, 0.9022316932678223, 0.8706679344177246, 0.8956876397132874, 0.8832741975784302, 0.9059213399887085, 0.9039567708969116, 0.9569481015205383, 0.8089810609817505, 0.9040949940681458, 0.9760062098503113, 0.9055288434028625, 0.9165382981300354, 0.8620995879173279, 0.9831612706184387, 0.9806168675422668, 0.8492785692214966, 0.8809232115745544, 0.9512019157409668, 0.8441895842552185, 0.8952348232269287, 0.855332612991333, 0.9663476347923279, 0.9192953109741211, 0.8654221296310425, 0.8659710884094238, 0.9332825541496277, 0.9336242079734802, 0.9329061508178711, 0.9700721502304077, 0.9767783880233765, 0.8525035381317139, 0.8794810771942139, 0.8374202847480774, 0.7924748063087463, 0.8611600995063782, 0.910315215587616, 0.8780158758163452, 0.8652121424674988, 0.8986475467681885, 0.9232815504074097, 0.966840922832489, 0.845363438129425, 0.872401237487793, 0.8790209293365479, 0.9494677782058716, 0.8902396559715271, 0.9291315078735352, 0.937680721282959, 0.8442533612251282, 0.8589931130409241, 0.8307337164878845, 0.8670648336410522, 0.8965488076210022, 0.8635802865028381, 0.9256805777549744, 0.951784610748291, 0.8691263794898987, 0.8885777592658997, 0.8428205251693726, 0.9151504039764404, 0.899197518825531, 0.846959114074707, 0.9283446073532104, 0.8366750478744507, 0.8914923071861267, 0.8679419755935669, 0.878957211971283, 0.9364599585533142, 0.9299477338790894, 0.9445511102676392, 0.9251941442489624, 0.9369769096374512, 0.8529787063598633, 0.8935896754264832, 0.8426973223686218, 0.9066495895385742, 0.8819825649261475, 0.9263715147972107, 0.9761419296264648, 0.9006781578063965, 0.9769068360328674, 0.849865198135376, 0.8149318695068359, 0.8920053839683533, 0.9224565625190735, 0.8608004450798035, 0.815990149974823, 0.8791028261184692, 0.8890965580940247, 0.8430041670799255, 0.8713059425354004, 0.9224517345428467, 0.8757516741752625, 0.8864050507545471, 0.8541132211685181, 0.8738482594490051, 0.9817295670509338, 0.8917940258979797, 0.8787223696708679, 0.9018338918685913, 0.9031781554222107, 0.8984532952308655, 0.8365628123283386, 0.892624020576477, 0.8608458042144775, 0.8497438430786133, 0.8975658416748047, 0.9003194570541382, 0.9022437930107117, 0.8854914903640747, 0.8587864637374878, 0.9169480204582214, 0.8656190037727356, 0.9374546408653259, 0.9114499092102051, 0.879957377910614, 0.8621190786361694, 0.8519832491874695, 0.8731614947319031, 0.8531438112258911, 0.8346924185752869, 0.8562483191490173, 0.8979044556617737, 0.8861751556396484, 0.9237059354782104, 0.9167345762252808, 0.8682522177696228, 0.896711528301239, 0.8846219778060913, 0.8522598743438721, 0.8705139756202698, 0.8371369242668152, 0.8683932423591614, 0.9004648923873901, 0.8234983682632446, 0.9345102906227112, 0.8460079431533813, 0.8264673352241516, 0.8363863229751587, 0.8997166156768799, 0.8989336490631104, 0.906993567943573, 0.845000147819519, 0.8346183896064758, 0.7773752212524414, 0.8636741638183594, 0.8598321676254272, 0.8422379493713379, 0.918375551700592, 0.8802556395530701, 0.8619398474693298, 0.8785161972045898, 0.8924431800842285, 0.8279238939285278, 0.8136976361274719, 0.8280048370361328, 0.8705854415893555, 0.9074662327766418, 0.9532777070999146, 0.8317935466766357, 0.924128532409668, 0.9395292401313782, 0.9068571329116821, 0.9217934012413025, 0.9098311066627502, 0.9388471245765686, 0.8823795318603516, 0.8781678676605225, 0.9173430800437927, 0.880391001701355, 0.8911039233207703, 0.8549928069114685, 0.9057429432868958, 0.8887685537338257, 0.923276424407959, 0.819042980670929, 0.9171528816223145, 0.8388766646385193, 0.893889844417572, 0.89363032579422, 0.8891641497612, 0.9113069176673889, 0.862479031085968, 0.8619000911712646, 0.9005264639854431, 0.882432758808136, 0.8311365842819214, 0.8559743165969849, 0.8199317455291748, 0.829034149646759, 0.8645930290222168, 0.8396468758583069, 0.8081485033035278, 0.9107959866523743, 0.9083691239356995, 0.9296171069145203, 0.8636897206306458, 0.8765742182731628, 0.9087260961532593, 0.9537370800971985, 0.8493509292602539, 0.868117094039917, 0.8861806988716125, 0.8505905270576477, 0.9086401462554932, 0.8268919587135315, 0.8550713062286377, 0.8687384724617004, 0.9013664722442627, 0.8893122673034668, 0.8707506656646729, 0.909724771976471, 0.9573594927787781, 0.8919020295143127, 0.9393945336341858, 0.8917741179466248, 0.9062919020652771, 0.9388962984085083, 0.8670971393585205, 0.9034275412559509, 0.8924891948699951, 0.9274064302444458, 0.8790101408958435, 0.8883329033851624, 0.8534460663795471, 0.9319337606430054, 0.8898822069168091, 0.8392871022224426, 0.8274476528167725, 0.8467429280281067, 0.8355465531349182, 0.8875171542167664, 0.942412257194519, 0.8506038188934326, 0.9045719504356384, 0.8779672384262085, 0.8760151863098145, 0.8746703267097473, 0.8706753849983215, 0.9178797602653503, 0.8784326314926147, 0.8763709664344788, 0.8428049683570862, 0.8547061681747437, 0.8757781386375427, 0.8215864896774292, 0.8737919926643372, 0.8876581192016602, 0.8674444556236267, 0.8609335422515869, 0.9033904671669006, 0.9296945929527283, 0.8780874609947205, 0.8249842524528503, 0.880761444568634, 0.8680502772331238, 0.8319251537322998, 0.8531914353370667, 0.8423053622245789, 0.8571282029151917, 0.8314303159713745, 0.866889476776123, 0.8665584325790405, 0.8022191524505615, 0.849017322063446, 0.8641984462738037, 0.8494545817375183, 0.8138493895530701, 0.9288597702980042, 0.8825228214263916, 0.904110848903656, 0.8701966404914856, 0.946397066116333, 0.9857947826385498, 0.8832689523696899, 0.9590771198272705, 0.8805199265480042, 0.8734183311462402, 0.8712920546531677, 0.9334119558334351, 0.9079499840736389, 0.9618659615516663, 0.874880313873291, 0.9135785102844238, 0.9519026875495911, 0.9638025760650635, 0.9334104657173157, 0.8794670701026917, 0.9380465149879456, 0.9278155565261841, 0.8823225498199463, 0.9470786452293396, 0.9057530164718628, 0.9152505993843079, 0.8608964681625366, 0.8784939050674438, 0.8519502878189087, 0.9126417636871338, 0.9605799317359924, 0.9040328860282898, 0.8471699357032776, 0.8510072231292725, 0.8275430202484131, 0.8531211018562317, 0.9190636277198792, 0.9325671195983887, 0.8517457842826843, 0.8854774832725525, 0.8854037523269653, 0.8832560777664185, 0.8808087110519409, 0.8765110969543457, 0.8652032017707825, 0.9483317732810974, 0.8590327501296997, 0.8743979334831238, 0.8910714387893677, 0.948420524597168, 0.9646698236465454, 0.8565421104431152, 0.8878329992294312, 0.8426465392112732, 0.9225936532020569, 0.9041324257850647, 0.9686629176139832, 0.9563317894935608, 0.9392955899238586, 0.9621702432632446, 0.891973078250885, 0.8761981129646301, 0.8459005951881409, 0.8998312950134277, 0.8210505247116089, 0.8785759806632996, 0.9327739477157593, 0.8599294424057007, 0.8349654078483582, 0.9119666218757629, 0.9339268207550049, 0.8850980997085571, 0.8614519834518433, 0.8806275129318237, 0.9093106389045715, 0.8433018326759338, 0.941346287727356, 0.8370160460472107, 0.9008642435073853, 0.8665273785591125, 0.8487656712532043, 0.9180508852005005, 0.8429826498031616, 0.8315731883049011, 0.8564725518226624, 0.9088481068611145, 0.9118407368659973, 0.8716046810150146, 0.8447612524032593, 0.8913132548332214, 0.9072964787483215, 0.898124098777771, 0.8897612690925598, 0.8772268295288086, 0.8631342053413391, 0.8399834632873535, 0.8816391825675964, 0.7701503038406372, 0.8572085499763489, 0.7986106872558594, 0.8193840980529785, 0.8702503442764282, 0.8814394474029541, 0.8518069982528687, 0.8304991126060486, 0.8309140205383301, 0.8689258694648743, 0.8574438095092773, 0.8552088737487793, 0.8745222091674805, 0.8344303369522095, 0.8209455013275146, 0.9118744134902954, 0.9208862781524658, 0.8372222781181335, 0.8450425267219543, 0.8455740809440613, 0.8634213805198669, 0.8416897058486938, 0.8499635457992554, 0.8388710618019104, 0.8570095896720886, 0.8833057284355164, 0.8763135671615601, 0.7750427722930908, 0.8401976227760315, 0.8249515891075134, 0.8848708868026733, 0.8521773219108582, 0.8808398246765137, 0.8664263486862183, 0.9089640378952026, 0.8658447265625, 0.8627983927726746, 0.8302562236785889, 0.9399705529212952, 0.9458591341972351, 0.880348801612854, 0.8615871071815491, 0.8771789073944092, 0.9035530686378479, 0.8619173169136047, 0.8671422004699707, 0.8531139492988586, 0.8518712520599365, 0.8499314785003662, 0.8645450472831726, 0.8661501407623291, 0.8831837773323059, 0.855191707611084, 0.8474867343902588, 0.8555147647857666, 0.9045740962028503, 0.8793178796768188, 0.8450113534927368, 0.9016230702400208, 0.873947262763977, 0.9010844826698303, 0.8740957379341125, 0.8990611433982849, 0.8590525388717651, 0.8740761280059814, 0.9293034076690674, 0.8659628629684448, 0.8967657089233398, 0.9171609878540039, 0.8956905007362366, 0.9206802248954773, 0.8996358513832092, 0.8777012825012207, 0.9547897577285767, 0.9763193130493164, 0.8487370014190674, 0.8906086087226868, 0.876522958278656, 0.8454862236976624, 0.87687748670578, 0.8934075832366943, 0.8448567390441895, 0.9108824729919434, 0.9191965460777283, 0.8640869855880737, 0.8986718654632568, 0.8570359349250793, 0.8561160564422607, 0.8796660900115967, 0.888308584690094, 0.9022172093391418, 0.8991541266441345, 0.8844771981239319, 0.8654297590255737, 0.9638276100158691, 0.8494976162910461, 0.9503109455108643, 0.9289854764938354, 0.8778998851776123, 0.859734833240509, 0.8804703950881958, 0.7815535068511963, 0.8549260497093201, 0.8395828604698181, 0.883205771446228, 0.9172582030296326, 0.9311609268188477, 0.9149841666221619, 0.8949987292289734, 0.9135271310806274, 0.8802574276924133, 0.8453923463821411, 0.90104079246521, 0.8782613277435303, 0.863603949546814, 0.873127818107605, 0.863483190536499, 0.8866625428199768, 0.9180475473403931, 0.8283464908599854, 0.9263212084770203, 0.9176722764968872, 0.8829867839813232, 0.8563506603240967, 0.8855528831481934, 0.8813160061836243, 0.9183875322341919, 0.9248788952827454, 0.879112184047699, 0.8235098719596863, 0.780048131942749, 0.824522852897644, 0.8700122237205505, 0.7519800066947937, 0.757108211517334, 0.8720700740814209, 0.9538922905921936, 0.7616958022117615, 0.8321765065193176, 0.906220555305481, 0.8606838583946228, 0.8373360633850098, 0.8709165453910828, 0.885783851146698, 0.9512715339660645, 0.8781787157058716, 0.9595767259597778, 0.9461439847946167, 0.8995397090911865, 0.8774990439414978, 0.8759313225746155, 0.8225739002227783, 0.8723558187484741, 0.8872672319412231, 0.8814582228660583, 0.8606417775154114, 0.9137786030769348, 0.8585093021392822, 0.8897855281829834, 0.918273389339447, 0.8202767968177795, 0.8206468224525452, 0.8936614394187927, 0.9183224439620972, 0.9026792049407959, 0.8588976263999939, 0.9076147675514221, 0.907160222530365, 0.8421758413314819, 0.8634900450706482, 0.8245575428009033, 0.8289859294891357, 0.9049643874168396, 0.8884204030036926, 0.8702477812767029, 0.9286763072013855, 0.8665710091590881, 0.8856716752052307, 0.8493122458457947, 0.8452811241149902, 0.8797587752342224, 0.8688676953315735, 0.863070011138916, 0.8656348586082458, 0.8524860739707947, 0.8570243716239929, 0.8535202145576477, 0.853447437286377, 0.9064980745315552, 0.9076555371284485, 0.9352940917015076, 0.8515912890434265, 0.8982034921646118, 0.8707000613212585, 0.8561121225357056, 0.8658774495124817, 0.8061602115631104, 0.8656825423240662, 0.8530176281929016, 0.8782577514648438, 0.9181897640228271, 0.907650351524353, 0.8737421035766602, 0.8309636116027832, 0.8826722502708435, 0.8792873620986938, 0.8395728468894958, 0.8641761541366577, 0.8150960206985474, 0.796371579170227, 0.8930041790008545, 0.8689525127410889, 0.8660244941711426, 0.8520854115486145, 0.8544207215309143, 0.9349626302719116, 0.8712106347084045, 0.8907149434089661, 0.8990429639816284, 0.8690609335899353, 0.8696926832199097, 0.8805643320083618, 0.8680248260498047, 0.8408588767051697, 0.8761923313140869, 0.8907844424247742, 0.8680529594421387, 0.8879087567329407, 0.8175804018974304, 0.9124670624732971, 0.8642560839653015, 0.8255430459976196, 0.9296281933784485, 0.9128987789154053, 0.8531362414360046, 0.8923333883285522, 0.889797568321228, 0.9210783243179321, 0.8687387108802795, 0.8503633141517639, 0.833465576171875, 0.8576658964157104, 0.8404421210289001, 0.8433742523193359, 0.9152817726135254, 0.8513067364692688, 0.8944472670555115, 0.8729714751243591, 0.8679417967796326, 0.8485120534896851, 0.8528457283973694, 0.8621113896369934, 0.865546464920044, 0.8798556327819824, 0.8883171677589417, 0.872840404510498, 0.8808208107948303, 0.8396920561790466, 0.9490887522697449, 0.87647545337677, 0.9510347843170166, 0.8910441994667053, 0.8263395428657532, 0.887550950050354, 0.8711625933647156, 0.8450295329093933, 0.8718709349632263, 0.8310967683792114, 0.8471678495407104, 0.8683436512947083, 0.908584475517273, 0.8582321405410767, 0.8665333390235901, 0.9203018546104431, 0.8898656964302063, 0.9000037312507629, 0.9750763177871704, 0.8651345372200012, 0.8613179326057434, 0.944210410118103, 0.9175093173980713, 0.8280392289161682, 0.897150993347168, 0.8986936807632446, 0.8494214415550232, 0.9135451912879944, 0.9250989556312561, 0.8877488374710083, 0.9194398522377014, 0.8238399028778076, 0.8781481385231018, 0.8708546161651611, 0.8898553252220154, 0.9479182958602905, 0.8423581123352051, 0.8263975977897644, 0.829741895198822, 0.9129205346107483, 0.9662491679191589, 0.9041663408279419, 0.8650296330451965, 0.8044923543930054, 0.864402174949646, 0.8115684390068054, 0.9103877544403076, 0.8685201406478882, 0.8805426359176636, 0.8402372598648071, 0.8962448835372925, 0.8855592012405396, 0.9559713006019592, 0.8864375948905945, 0.89708411693573, 0.8836256265640259, 0.8679724335670471, 0.8697155714035034, 0.9268884062767029, 0.8697670698165894, 0.9160054922103882, 0.9037512540817261, 0.8530659675598145, 0.8648737668991089, 0.9122110605239868, 0.8535652160644531, 0.9001702070236206, 0.8398566246032715, 0.9208394289016724, 0.9066693782806396, 0.8192859888076782, 0.838709831237793, 0.8797951340675354, 0.8324400186538696, 0.8925692439079285, 0.895684540271759, 0.9625083804130554, 0.8805176019668579, 0.8040807843208313, 0.9004413485527039, 0.7928799986839294, 0.8910435438156128, 0.8873938322067261, 0.8691991567611694, 0.8787534236907959, 0.8457304239273071, 0.8647372722625732, 0.9450806379318237, 0.8336183428764343, 0.8852168321609497, 0.9832837581634521, 0.8387322425842285, 0.8684741258621216, 0.9453323483467102, 0.8840309381484985, 0.8843849897384644, 0.8925261497497559, 0.8776328563690186, 0.9362747073173523, 0.8839961886405945, 0.856666624546051, 0.9589527249336243, 0.9645693898200989, 0.8955590724945068, 0.8763405680656433, 0.8265736103057861, 0.9492582678794861, 0.9283116459846497, 0.8609915375709534, 0.8946644067764282, 0.9326261878013611, 0.9115241765975952, 0.8883886337280273, 0.9127443432807922, 0.9422171711921692, 0.8499099016189575, 0.8153855204582214, 0.9402332305908203, 0.8793913125991821, 0.8602367043495178, 0.906872570514679, 0.8723881840705872, 0.9106348752975464, 0.8637426495552063, 0.832991361618042, 0.8743057250976562, 0.9225207567214966, 0.8491727113723755, 0.8967782258987427, 0.8971595168113708, 0.9306621551513672, 0.8670147657394409, 0.9577155113220215, 0.8746376037597656, 0.8796126246452332, 0.8672888875007629, 0.8858804702758789, 0.9168880581855774, 0.9201310873031616, 0.9389588236808777, 0.8291005492210388, 0.8857182264328003, 0.9650163650512695, 0.8521550893783569, 0.8806008696556091, 0.8867712616920471, 0.910815954208374, 0.9168922305107117, 0.9132128953933716, 0.9030294418334961, 0.9421452879905701, 0.8929409980773926, 0.9284642934799194, 0.8759640455245972, 0.8826799988746643, 0.8542968034744263, 0.8896418213844299, 0.8693018555641174, 0.9212656021118164, 0.8536316156387329, 0.8726108074188232, 0.8744757175445557, 0.8887777924537659, 0.8563029170036316, 0.9583805203437805, 0.8266347050666809, 0.8835602402687073, 0.8546993732452393, 0.8835085034370422, 0.9122223258018494, 0.8930811882019043, 0.8836243152618408, 0.915627121925354, 0.812747061252594, 0.873284101486206, 0.8890764713287354, 0.8411509990692139, 0.8649651408195496, 0.905820906162262, 0.9159241318702698, 0.9611815214157104, 0.887321949005127, 0.9029405117034912, 0.8990530967712402, 0.8991792798042297, 0.9565724730491638, 0.8778899312019348, 0.899571418762207, 0.8741381764411926, 0.9093523025512695, 0.9190987944602966, 0.8534559011459351, 0.8942026495933533, 0.8837468028068542, 0.8990523815155029, 0.8782958388328552, 0.9820836782455444, 0.8434755802154541, 0.8871850967407227, 0.9606446027755737, 0.9034510850906372, 0.9075766801834106, 0.9499892592430115, 0.8881382346153259, 0.9086018800735474, 0.877814769744873, 0.8627836108207703, 0.8739029169082642, 0.8771194815635681, 0.8531231880187988, 0.8099886775016785, 0.8449342250823975, 0.7759730219841003, 0.8518415689468384, 0.9037286639213562, 0.9257513880729675, 0.8655209541320801, 0.8173253536224365, 0.9164353609085083, 0.880571186542511, 0.8896456360816956, 0.8549198508262634, 0.8899210095405579, 0.8961283564567566, 0.8402277231216431, 0.8389025926589966, 0.8747383952140808, 0.871059775352478, 0.8840983510017395, 0.9211060404777527, 0.8558695912361145, 0.8319960832595825, 0.9234070181846619, 0.8747766017913818, 0.8510109782218933, 0.8468555212020874, 0.9091471433639526, 0.8907347321510315, 0.8843339681625366, 0.9070219397544861, 0.8466826677322388, 0.8323898911476135, 0.9312101006507874, 0.8755808472633362, 0.8880351185798645, 0.8802641034126282, 0.890008270740509, 0.8413013219833374, 0.882313072681427, 0.8488245010375977, 0.823951780796051, 0.9078338146209717, 0.9060553312301636, 0.8870794773101807, 0.8415103554725647, 0.928019106388092, 0.8160107135772705, 0.8636154532432556, 0.9463412761688232, 0.8594874143600464, 0.941372275352478, 0.859931230545044, 0.8194066286087036, 0.9202728867530823, 0.9369840025901794, 0.8914673328399658, 0.851932942867279, 0.8053112030029297, 0.9041369557380676, 0.8694064617156982, 0.8426371216773987, 0.9398748874664307, 0.8116655349731445, 0.8454872965812683, 0.8045253157615662, 0.8773347735404968, 0.8517457842826843, 0.8638616800308228, 0.8847966194152832, 0.9091091752052307, 0.8569204211235046, 0.8521854877471924, 0.8763553500175476, 0.9282670617103577, 0.8122681379318237, 0.8236545324325562, 0.8336484432220459, 0.9169912338256836, 0.8425581455230713, 0.8356549143791199, 0.8693216443061829, 0.8539518117904663, 0.8717557191848755, 0.8789535760879517, 0.8856672644615173, 0.8692446351051331, 0.8755597472190857, 0.8347740173339844, 0.9050118923187256, 0.8736461997032166, 0.8453919887542725, 0.8284640312194824, 0.8441116213798523, 0.888543426990509, 0.8712300658226013, 0.8710238933563232, 0.8795819282531738, 0.9019297957420349, 0.837526798248291, 0.9467130899429321, 0.8683498501777649, 0.9109750986099243, 0.9591783881187439, 0.9539982676506042, 0.8208332657814026, 0.8385676145553589, 0.8866007328033447, 0.8281692862510681, 0.9559838175773621, 0.8304078578948975, 0.9010260105133057, 0.8798370957374573, 0.8190675973892212, 0.8619057536125183, 0.8862411975860596, 0.8653488755226135, 0.9138373136520386, 0.9020291566848755, 0.8982840776443481, 0.9626485109329224, 0.8992599844932556, 0.9210683107376099, 0.8547081351280212, 0.8575083613395691, 0.8541614413261414, 0.9167666435241699, 0.881164014339447, 0.8655120134353638, 0.8260217308998108, 0.8507726788520813, 0.8808290958404541, 0.8881721496582031, 0.9362537860870361, 0.9324407577514648, 0.8683652281761169, 0.8521025776863098, 0.9178505539894104, 0.863635241985321, 0.8209772109985352, 0.8549167513847351, 0.916533350944519, 0.8656994700431824, 0.8704579472541809, 0.8684585094451904, 0.8852073550224304, 0.8611281514167786, 0.935286283493042, 0.8769809007644653, 0.8463909029960632, 0.9290974140167236, 0.9589252471923828, 0.8667565584182739, 0.8426197171211243, 0.8050750494003296, 0.8225260972976685, 0.947274923324585, 0.8724043369293213, 0.8083852529525757, 0.8894407749176025, 0.8862085342407227, 0.8481622338294983, 0.9157600402832031, 0.8754370212554932, 0.8135014176368713, 0.8414298295974731, 0.8816145062446594, 0.8460617661476135, 0.8892402648925781, 0.8787598013877869, 0.8766092658042908, 0.8488778471946716, 0.8435271382331848, 0.8608641624450684, 0.8652798533439636, 0.8707505464553833, 0.8855916261672974, 0.8577440977096558, 0.940302312374115, 0.9494333863258362, 0.8220758438110352, 0.8910954594612122, 0.8567472100257874, 0.8406459093093872, 0.8956543803215027, 0.9193412065505981, 0.8758560419082642, 0.9018727540969849, 0.8486483693122864, 0.8646346926689148, 0.9053670763969421, 0.871839702129364, 0.9190791249275208, 0.8895266652107239, 0.8546977639198303, 0.8768545985221863, 0.8749867677688599, 0.970446765422821, 0.8547596335411072, 0.8445104956626892, 0.8503939509391785, 0.876798152923584, 0.9276036024093628, 0.8549201488494873, 0.9111486077308655, 0.8537523150444031, 0.8369854688644409, 0.9120672941207886, 0.8647798299789429, 0.9603234529495239, 0.8725978136062622, 0.8516820669174194, 0.8699578046798706, 0.8826308846473694, 0.8764958381652832, 0.9166648983955383, 0.8137773871421814, 0.8365856409072876, 0.8242746591567993, 0.8788143396377563, 0.8943501114845276, 0.9153968691825867, 0.8995350003242493, 0.890818178653717, 0.8649531006813049, 0.8720543384552002, 0.9447450637817383, 0.8525770306587219, 0.9335846304893494, 0.9488726854324341, 0.858618438243866, 0.8801892399787903, 0.8417423963546753, 0.894618809223175, 0.9231151938438416, 0.9618011116981506, 0.9209681153297424, 0.9230047464370728, 0.9177221059799194, 0.8875475525856018, 0.8885287046432495, 0.9263390302658081, 0.9113710522651672, 0.9653130173683167, 0.9071242809295654, 0.8668670654296875, 0.9317320585250854, 0.8794506192207336, 0.8193027377128601, 0.8486118316650391, 0.8699718713760376, 0.8874301910400391, 0.9786033034324646, 0.9480109214782715, 0.8450341820716858, 0.9079756736755371, 0.8573957681655884, 0.9054961800575256, 0.9372251033782959, 0.8635337948799133, 0.8960030674934387, 0.9216967821121216, 0.9075674414634705, 0.9400600790977478, 0.875858724117279, 0.8796568512916565, 0.87888503074646, 0.8735219836235046, 0.9580647349357605, 0.8598954677581787, 0.8848435878753662, 0.8690072298049927, 0.8603001832962036, 0.7968431115150452, 0.8881320357322693, 0.8767403364181519, 0.8847172856330872, 0.867946207523346, 0.8608108758926392, 0.9119541049003601, 0.9118799567222595, 0.8948159217834473, 0.9628949761390686, 0.8716187477111816, 0.8459851741790771, 0.8721765875816345, 0.9131607413291931, 0.873027503490448, 0.9085360169410706, 0.8709537982940674, 0.9147562384605408, 0.8602961897850037, 0.9062560200691223, 0.8749331831932068, 0.9377029538154602, 0.8964219093322754, 0.8593981266021729, 0.8788263201713562, 0.8526981472969055, 0.8439735770225525, 0.901771068572998, 0.8846307396888733, 0.8668801188468933, 0.94029301404953, 0.9188463091850281, 0.9046913981437683, 0.882777750492096, 0.8958288431167603, 0.841410219669342, 0.8665082454681396, 0.9175638556480408, 0.9519802927970886, 0.8491738438606262, 0.8464939594268799, 0.9222833514213562, 0.8662543296813965, 0.9307581782341003, 0.8858806490898132, 0.8433646559715271, 0.9334503412246704, 0.8338821530342102, 0.874195396900177, 0.968448281288147, 0.9199985265731812, 0.8648292422294617, 0.8979514837265015, 0.9282646775245667, 0.9160088896751404, 0.8827486038208008, 0.8856903910636902, 0.8843957781791687, 0.837909996509552, 0.8672513365745544, 0.834333062171936, 0.8413228988647461, 0.8319922685623169, 0.8807640075683594, 0.883017361164093, 0.9100472331047058, 0.9139087796211243, 0.8859167695045471, 0.9004256725311279, 0.9156691431999207, 0.8500903844833374, 0.8419725298881531, 0.9023557305335999, 0.8955848813056946, 0.9081915020942688, 0.8605849742889404, 0.8293837308883667, 0.9554629921913147, 0.89370197057724, 0.8868532776832581, 0.8879695534706116, 0.8486558198928833, 0.9054135680198669, 0.932989239692688, 0.8695488572120667, 0.8595514297485352, 0.8496003746986389, 0.9504006505012512, 0.930864691734314, 0.8378236293792725, 0.8472705483436584, 0.8760493397712708, 0.8868309855461121, 0.8500004410743713, 0.9187830090522766, 0.9026289582252502, 0.8864370584487915, 0.918156623840332, 0.8993226289749146, 0.8701322674751282, 0.9141063094139099, 0.8853628635406494, 0.828499436378479, 0.858663022518158, 0.9637416005134583, 0.87382972240448, 0.8734700679779053, 0.873848557472229, 0.9328057169914246, 0.8812965750694275, 0.8606934547424316, 0.8683298230171204, 0.9144540429115295, 0.8520281910896301, 0.9080111980438232, 0.8503492474555969, 0.9337598085403442, 0.8406532406806946, 0.8473861813545227, 0.8965861201286316, 0.8840407729148865, 0.8873018622398376, 0.86390221118927, 0.8758985996246338, 0.9040796756744385, 0.9692063927650452, 0.8250679969787598, 0.9189995527267456, 0.850561261177063, 0.8940903544425964, 0.884233832359314, 0.8824466466903687, 0.8688014149665833, 0.9292106032371521, 0.8509740829467773, 0.8927568197250366, 0.8867636919021606, 0.8944157361984253, 0.8744399547576904, 0.9426372051239014, 0.8556082248687744, 0.9270535707473755, 0.9111282229423523, 0.9419331550598145, 0.8989815711975098, 0.8778347373008728, 0.8342167735099792, 0.8685510754585266, 0.8749103546142578, 0.9445871710777283, 0.8643817901611328, 0.897261917591095, 0.9101283550262451, 0.8040111660957336, 0.8926975131034851, 0.8736175894737244, 0.8647529482841492, 0.9083672165870667, 0.8554320931434631, 0.8867302536964417, 0.886611819267273, 0.8768495917320251, 0.912449300289154, 0.8294895887374878, 0.8424387574195862, 0.8848830461502075, 0.8702989220619202, 0.8641401529312134, 0.8831996917724609, 0.8635018467903137, 0.9225582480430603, 0.8544825911521912, 0.9006751775741577, 0.9129094481468201, 0.852665364742279, 0.8753681778907776, 0.9621998071670532, 0.921575665473938, 0.8891337513923645, 0.8679994940757751, 0.8252613544464111, 0.8670838475227356, 0.8718045949935913, 0.8240082263946533, 0.8388388752937317, 0.8674307465553284, 0.8823210000991821, 0.8226472735404968, 0.8792244791984558, 0.9207696914672852, 0.842889666557312, 0.8617765307426453, 0.868297815322876, 0.863282322883606, 0.9404526352882385, 0.8115050792694092, 0.8474635481834412, 0.8102805018424988, 0.8669363260269165, 0.9200602769851685, 0.8754789233207703, 0.8938046097755432, 0.8446436524391174, 0.8794397711753845, 0.9227374196052551, 0.839333713054657, 0.8775231242179871, 0.8668349981307983, 0.8695002198219299, 0.8447953462600708, 0.8856557011604309, 0.8722269535064697, 0.8046352863311768, 0.928349494934082, 0.828967809677124, 0.8535882830619812, 0.8612285256385803, 0.8299176692962646, 0.8752306699752808, 0.8329796195030212, 0.9288719296455383, 0.9706577658653259, 0.8705946803092957, 0.9618138670921326, 0.8716368079185486, 0.8750917911529541, 0.8327189087867737, 0.8795361518859863, 0.8088380098342896, 0.7967982292175293, 0.8736997246742249, 0.8567354679107666, 0.8800461292266846, 0.8463857769966125, 0.8799667358398438, 0.8865247368812561, 0.8876096606254578, 0.933681309223175, 0.8485796451568604, 0.8757519125938416, 0.8802518248558044, 0.888464093208313, 0.8648446798324585, 0.9269384145736694, 0.8466924428939819, 0.9062842130661011, 0.8586194515228271, 0.8706104159355164, 0.9233758449554443, 0.8864635229110718, 0.895402193069458, 0.8707414865493774, 0.8495473265647888, 0.8997386693954468, 0.843014121055603, 0.9141542911529541, 0.84683758020401, 0.8544326424598694, 0.9008228182792664, 0.9040474891662598, 0.921638011932373, 0.9127053022384644, 0.9756422638893127, 0.8765756487846375, 0.8790201544761658, 0.9039551615715027, 0.8473776578903198, 0.823624312877655, 0.8703888058662415, 0.919370710849762, 0.8496856093406677, 0.9365949034690857, 0.8619747757911682, 0.8914816975593567, 0.8834685683250427, 0.9287713170051575, 0.8990839719772339, 0.9298924207687378, 0.7974679470062256, 0.8866074085235596, 0.8412579894065857, 0.9032443761825562, 0.9296437501907349, 0.871222734451294, 0.8615562915802002, 0.8809270858764648, 0.9545480608940125, 0.8961613178253174, 0.8804112672805786, 0.8993913531303406, 0.8229902982711792, 0.9082873463630676, 0.8863479495048523, 0.8806074857711792, 0.8691259026527405, 0.8601852059364319, 0.8203309178352356, 0.9321871399879456, 0.9457684755325317, 0.799145519733429, 0.8891839981079102, 0.9297769665718079, 0.8835694193840027, 0.8745687007904053, 0.880449116230011, 0.9289174675941467, 0.8672885894775391, 0.8832845091819763, 0.8328225016593933, 0.8782004714012146, 0.8367691040039062, 0.84207683801651, 0.8423013091087341, 0.9405375123023987, 0.9291650056838989, 0.8944225311279297, 0.9437220692634583, 0.865585446357727, 0.8428218364715576, 0.8499695062637329, 0.8830420970916748, 0.8316476345062256, 0.8736335039138794, 0.8305417895317078, 0.8491246700286865, 0.8461445569992065, 0.8771648406982422, 0.8897033929824829, 0.9151598215103149, 0.8661161661148071, 0.8685076236724854, 0.9261436462402344, 0.8790631294250488, 0.8965750336647034, 0.9090853333473206, 0.8668022155761719, 0.8725019693374634, 0.9076662063598633, 0.8642622828483582, 0.8665871620178223, 0.9061137437820435, 0.9674486517906189, 0.8297600150108337, 0.8519492149353027, 0.865982711315155, 0.9303072094917297, 0.8847150206565857, 0.8234409093856812, 0.8745139837265015, 0.821583092212677, 0.8528786301612854, 0.9001105427742004, 0.8238463997840881, 0.8919879198074341, 0.8672375679016113, 0.831274688243866, 0.9001418948173523, 0.8820646405220032, 0.890392005443573, 0.8966360092163086, 0.9068521857261658, 0.8713083267211914, 0.8675662875175476, 0.886957585811615, 0.8630772233009338, 0.8886628746986389, 0.8789750337600708, 0.8437214493751526, 0.839283287525177, 0.8916788101196289, 0.841970682144165, 0.8868681788444519, 0.9196490049362183, 0.8438274264335632, 0.9192371368408203, 0.8193046450614929, 0.8831683397293091, 0.8723581433296204, 0.9405236840248108, 0.9553497433662415, 0.8971620798110962, 0.9406588077545166, 0.9811399579048157, 0.8783262968063354, 0.8805241584777832, 0.8817747235298157, 0.8763819932937622, 0.8168213963508606, 0.9291194677352905, 0.8555076718330383, 0.906000554561615, 0.9133084416389465, 0.8970988392829895, 0.9108090400695801, 0.9239679574966431, 0.8847261071205139, 0.8294325470924377, 0.8776088953018188, 0.8797376751899719, 0.8960378170013428, 0.9088557958602905, 0.9353256821632385, 0.852640688419342, 0.8935992121696472, 0.8707584142684937, 0.8422268629074097, 0.9277076125144958, 0.914385974407196, 0.8225488066673279, 0.8611766695976257, 0.9078716039657593, 0.8926430344581604, 0.8965204954147339, 0.9255469441413879, 0.9369310736656189, 0.8626843690872192, 0.8830937147140503, 0.9525736570358276, 0.8740013241767883, 0.93282550573349, 0.907630443572998, 0.9582003355026245, 0.9603053331375122, 0.8915634155273438, 0.9081705808639526, 0.9136711359024048, 0.9137081503868103, 0.9272869825363159, 0.8813793063163757, 0.8390840888023376, 0.9290351867675781, 0.9126437902450562, 0.8711526989936829, 0.8646968007087708, 0.8608635067939758, 0.9274821281433105, 0.8568364977836609, 0.9419527053833008, 0.9222444295883179, 0.8445582985877991, 0.8965041637420654, 0.9197001457214355, 0.8804530501365662, 0.9240056872367859, 0.9036127924919128, 0.8646798133850098, 0.8702998161315918, 0.9030275344848633, 0.9205434322357178, 0.8939439654350281, 0.9431663751602173, 0.9272846579551697, 0.9318616390228271, 0.9248090386390686, 0.8554546236991882, 0.9146285057067871, 0.8322494029998779, 0.8740856647491455, 0.9495688080787659, 0.9291486740112305, 0.9645507335662842, 0.8974208235740662, 0.8996177911758423, 0.862018346786499, 0.9453793168067932, 0.849256157875061, 0.833285391330719, 0.8378006815910339, 0.8418038487434387, 0.8771284222602844, 0.8566164970397949, 0.8689206838607788, 0.9022788405418396, 0.8486332893371582, 0.8935160040855408, 0.93845134973526, 0.8838381171226501, 0.8673062920570374, 0.9422413110733032, 0.8771572113037109, 0.916434109210968, 0.8607929348945618, 0.9006602168083191, 0.9434672594070435, 0.9394147396087646, 0.9174237847328186, 0.8873074650764465, 0.9130606651306152, 0.9476432204246521, 0.9016149044036865, 0.891227662563324, 0.9162929654121399, 0.8500340580940247, 0.9075818657875061, 0.8455113768577576, 0.8927426934242249, 0.8586137294769287, 0.8956596255302429, 0.9012953042984009, 0.9389174580574036, 0.903687059879303, 0.8997185230255127, 0.9584264755249023, 0.8942543268203735, 0.9381943941116333, 0.8822862505912781, 0.8808866739273071, 0.918910026550293, 0.893336832523346, 0.9072685241699219, 0.9131123423576355, 0.8507624268531799, 0.9569401144981384, 0.942714273929596, 0.9080970883369446, 0.9183394312858582, 0.8931730389595032, 0.8985572457313538, 0.8847268223762512, 0.9584363698959351, 0.8811202049255371, 0.8192786574363708, 0.9114947319030762, 0.8825804591178894, 0.852594256401062, 0.8544707298278809, 0.9163926839828491, 0.8751989006996155, 0.9072476029396057, 0.913769006729126, 0.925444483757019, 0.8490332365036011, 0.85882967710495, 0.9219569563865662, 0.8850528001785278, 0.8546058535575867, 0.8497053384780884, 0.8576704859733582, 0.8943378925323486, 0.8878815174102783, 0.882050633430481, 0.8366336226463318, 0.9085131287574768, 0.8982025980949402, 0.8955058455467224, 0.9470182657241821, 0.9050054550170898, 0.8511490225791931, 0.876447856426239, 0.8382841944694519, 0.8492896556854248, 0.8821576833724976, 0.9276586174964905, 0.899260938167572, 0.8873242735862732, 0.8620785474777222, 0.8558441996574402, 0.8718309998512268, 0.8906659483909607, 0.8339546322822571, 0.8344085216522217, 0.9309152364730835, 0.8666274547576904, 0.8672623038291931, 0.8855989575386047, 0.8489651679992676, 0.8948723077774048, 0.8742387890815735, 0.8492084741592407, 0.8294957280158997, 0.8506697416305542, 0.9468550086021423, 0.9544495344161987, 0.8792666792869568, 0.8307288885116577, 0.9342544674873352, 0.8621095418930054, 0.8696315884590149, 0.9251134991645813, 0.9665528535842896, 0.9417372345924377, 0.9450063705444336, 0.8574813604354858, 0.8732706308364868, 0.8477477431297302, 0.8784321546554565, 0.8713697791099548, 0.9284815192222595, 0.9250630736351013, 0.8799068331718445, 0.8325242400169373, 0.8111197352409363, 0.8319743275642395, 0.8065776228904724, 0.8350657820701599, 0.9451976418495178, 0.8859589099884033, 0.9178824424743652, 0.902609646320343, 0.9498128294944763, 0.9075554013252258, 0.9548380374908447, 0.94963538646698, 0.8584949374198914, 0.8549204468727112, 0.9094266891479492, 0.8864057064056396, 0.9111841917037964, 0.902668833732605, 0.9034026265144348, 0.8970471024513245, 0.8956843018531799, 0.8681284189224243, 0.8637735843658447, 0.90617436170578, 0.8698254823684692, 0.9110773205757141, 0.8905730843544006, 0.8622370958328247, 0.8814108967781067, 0.87224280834198, 0.8296059966087341, 0.9643018245697021, 0.8651984333992004, 0.866715669631958, 0.850606381893158, 0.8801096677780151, 0.8389886021614075, 0.8671875, 0.9195032119750977, 0.9467141628265381, 0.9797565340995789, 0.838506281375885, 0.8725704550743103, 0.894233763217926, 0.9724137187004089, 0.8747146129608154, 0.8956670165061951, 0.9374775886535645, 0.9408122897148132, 0.8812844157218933, 0.9179610013961792, 0.8163872361183167, 0.84466552734375, 0.8684192299842834, 0.8853816390037537, 0.9301729202270508, 0.8483402729034424, 0.8381645083427429, 0.8722493052482605, 0.7998126745223999, 0.8924360871315002, 0.867185115814209, 0.9065542817115784, 0.8958022594451904, 0.9053516983985901, 0.8630221486091614, 0.8413119316101074, 0.8913214206695557, 0.8248652219772339, 0.9001244306564331, 0.9091653823852539, 0.8665124773979187, 0.9504250288009644, 0.8755717873573303, 0.8403419852256775, 0.9157733917236328, 0.8874546885490417, 0.83994060754776, 0.8382782936096191, 0.9091848731040955, 0.8699057698249817, 0.8888149857521057, 0.8663930296897888, 0.8506261110305786, 0.9043911695480347, 0.9527273774147034, 0.9359127879142761, 0.9319367408752441, 0.8889936804771423, 0.8337583541870117, 0.843813955783844, 0.8646457195281982, 0.9113341569900513, 0.8594486117362976, 0.9100475907325745, 0.901658833026886, 0.8858022093772888, 0.8272100687026978, 0.8948670029640198, 0.9268853664398193, 0.8757558465003967, 0.9034143686294556, 0.8424946069717407, 0.9073337316513062, 0.8845210075378418, 0.8371677398681641, 0.8520171046257019, 0.871388852596283, 0.8336827158927917, 0.916397750377655, 0.8581026196479797, 0.9254046082496643, 0.9123169183731079, 0.9094899296760559, 0.8621572852134705, 0.7859294414520264, 0.871706485748291, 0.8433142900466919, 0.9115477204322815, 0.9420279860496521, 0.8579216599464417, 0.837904155254364, 0.8703708052635193, 0.8848857283592224, 0.876885175704956, 0.8649870753288269, 0.8841893672943115, 0.9642459154129028, 0.9413613080978394, 0.8696241974830627, 0.9059510231018066, 0.8704438209533691, 0.9324098229408264, 0.9015921950340271, 0.8546028137207031, 0.8571271896362305, 0.8927186727523804, 0.8826135993003845, 0.860866367816925, 0.8943343162536621, 0.8855164647102356, 0.8642272353172302, 0.8562682867050171, 0.9163733124732971, 0.8475531339645386, 0.913129985332489, 0.8584014177322388, 0.919147253036499, 0.8942621946334839, 0.8445525765419006, 0.8466399908065796, 0.8335767984390259, 0.8640535473823547, 0.8967660069465637, 0.8787620663642883, 0.8876807689666748, 0.8720028400421143, 0.8789491057395935, 0.8901808857917786, 0.886598527431488, 0.8537794947624207, 0.9146126508712769, 0.8973484635353088, 0.8894367218017578, 0.780053973197937, 0.8503562211990356, 0.9086840748786926, 0.848738968372345, 0.8754051327705383, 0.8607790470123291, 0.8681163787841797, 0.9570688605308533, 0.9074509143829346, 0.8477605581283569, 0.8629849553108215, 0.8869721293449402, 0.900711178779602, 0.8208172917366028, 0.8501005172729492, 0.8709775805473328, 0.8520532250404358, 0.8446423411369324, 0.8899234533309937, 0.8661013841629028, 0.8166722059249878, 0.9185324311256409, 0.8913037180900574, 0.9664029479026794, 0.8268017768859863, 0.8569262027740479, 0.9006636738777161, 0.8477250933647156, 0.8653438091278076, 0.8713001012802124, 0.904846727848053, 0.8998627066612244, 0.9008230566978455, 0.9361646175384521, 0.8692564368247986, 0.9200845956802368, 0.8022822141647339, 0.8515759110450745, 0.85642409324646, 0.8782986402511597, 0.8508228063583374, 0.8975796103477478, 0.9153947830200195, 0.8639911413192749, 0.9280577898025513, 0.8340467810630798, 0.8670036792755127, 0.8755809664726257, 0.8425936698913574, 0.9107482433319092, 0.9038472175598145, 0.8739452958106995, 0.8477837443351746, 0.8666083216667175, 0.889777421951294, 0.9256345629692078, 0.8693031072616577, 0.8666417598724365, 0.9675135016441345, 0.952730119228363, 0.9223328828811646, 0.8383095264434814, 0.8955758213996887, 0.8954727649688721, 0.8730586767196655, 0.8344182968139648, 0.8372106552124023, 0.8940216898918152, 0.8860638737678528, 0.8578270673751831, 0.8454994559288025, 0.8547766804695129, 0.9724597334861755, 0.9022373557090759, 0.8435960412025452, 0.8830592036247253, 0.8896482586860657, 0.8538267612457275, 0.8431050181388855, 0.8554184436798096, 0.823296844959259, 0.8952184915542603, 0.8913896083831787, 0.9308487176895142, 0.9685072302818298, 0.8588537573814392, 0.9295047521591187, 0.8392657041549683, 0.8511627316474915, 0.8465808629989624, 0.8538892865180969, 0.8391739726066589, 0.966257631778717, 0.8615191578865051, 0.8680824041366577, 0.8459734320640564, 0.8600465059280396, 0.9104459881782532, 0.8566925525665283, 0.8346274495124817, 0.7419499754905701, 0.9076362252235413, 0.8137578368186951, 0.8096356987953186, 0.8317487239837646, 0.9465141296386719, 0.8586333990097046, 0.7857933044433594, 0.909169614315033, 0.9510444402694702, 0.8855853080749512, 0.9090050458908081, 0.9419971108436584, 0.9139979481697083, 0.924196183681488, 0.8661981225013733, 0.8467997312545776, 0.8904943466186523, 0.9412147998809814, 0.8925813436508179, 0.9068104028701782, 0.947827935218811, 0.914583146572113, 0.8650596141815186, 0.8462404608726501, 0.8823219537734985, 0.8843991756439209, 0.9288003444671631, 0.8826339840888977, 0.8294351100921631, 0.8168757557868958, 0.8885177969932556, 0.8579104542732239, 0.8152534365653992, 0.8608272075653076, 0.899836540222168, 0.8541130423545837, 0.9065386056900024, 0.8692111968994141, 0.8922296762466431, 0.7916485667228699, 0.947961688041687, 0.9338920712471008, 0.9212770462036133, 0.7833824753761292, 0.8888899683952332, 0.88247150182724, 0.8422155380249023, 0.8835206031799316, 0.8800577521324158, 0.8856461644172668, 0.8373095989227295, 0.9216927886009216, 0.9215881824493408, 0.8453351259231567, 0.9463715553283691, 0.8528287410736084, 0.8446282148361206, 0.9251304268836975, 0.8726864457130432, 0.8908917903900146, 0.8712440133094788, 0.8845673203468323, 0.939158022403717, 0.9214953184127808, 0.8626499176025391, 0.7676068544387817, 0.8614891171455383, 0.9782747626304626, 0.8590471148490906, 0.9654480814933777, 0.8925963640213013, 0.9091558456420898, 0.8696323037147522, 0.9538230299949646, 0.9645700454711914, 0.884093701839447, 0.8988858461380005, 0.9433254599571228, 0.9541162848472595, 0.8974660634994507, 0.925841212272644, 0.8988537192344666, 0.9357911944389343, 0.9192167520523071, 0.9487225413322449, 0.916701078414917, 0.9147037863731384, 0.9521945118904114, 0.9384585618972778, 0.8911901712417603, 0.9773707985877991, 0.9022348523139954, 0.9205527901649475, 0.9125624895095825, 0.9502221345901489, 0.9590657949447632, 0.9738064408302307, 0.8894132375717163, 0.9228782057762146, 0.8449899554252625, 0.902009904384613, 0.8348394632339478, 0.8800504207611084, 0.8941547274589539, 0.908831000328064, 0.8655264973640442, 0.8560644388198853, 0.9440876841545105, 0.9146314859390259, 0.8680663704872131, 0.8991124629974365, 0.9006568789482117, 0.8554326295852661, 0.8297902345657349, 0.8647122979164124, 0.8298085331916809, 0.8843794465065002, 0.9709559082984924, 0.8860193490982056, 0.9702374935150146, 0.8971076011657715, 0.9414423108100891, 0.9441038370132446, 0.8647586703300476, 0.8515990376472473, 0.90459144115448, 0.8940163850784302, 0.861295759677887, 0.890979528427124, 0.8727174997329712, 0.9419077634811401, 0.8755557537078857, 0.9385359287261963, 0.8994569182395935, 0.8987528085708618, 0.9247910380363464, 0.9076683521270752, 0.9618991613388062, 0.8420836925506592, 0.8538416028022766, 0.9816609025001526, 0.9801685810089111, 0.9056154489517212, 0.9215860962867737, 0.8965809941291809, 0.9319884777069092, 0.9045318365097046, 0.9589920043945312, 0.9574270248413086, 0.9346709251403809, 0.8597638607025146, 0.846223771572113, 0.9002248048782349, 0.9272906184196472, 0.8568530082702637, 0.8306951522827148, 0.9026365280151367, 0.8687599301338196, 0.8105006814002991, 0.7815244197845459, 0.796213686466217, 0.8525426983833313, 0.8567944169044495, 0.8837071657180786, 0.8661030530929565, 0.8693941831588745, 0.9341234564781189, 0.8529804944992065, 0.9437170028686523, 0.8989360332489014, 0.8832654356956482, 0.8785175681114197, 0.9331137537956238, 0.8613697290420532, 0.8828784227371216, 0.8783136606216431, 0.8670846819877625, 0.8716978430747986, 0.9173930287361145, 0.8552185297012329, 0.9098461270332336, 0.9119067192077637, 0.8670557737350464, 0.8445228338241577, 0.8589114546775818, 0.8478798866271973, 0.8242360949516296, 0.8545264601707458, 0.906626284122467, 0.9541305303573608, 0.9392780065536499, 0.8614243865013123, 0.9126237034797668, 0.8227202296257019, 0.945129930973053, 0.8551809191703796, 0.8662350177764893, 0.8831616044044495, 0.8961929678916931, 0.9039749503135681, 0.840964674949646, 0.9300794005393982, 0.9441230893135071, 0.8807999491691589, 0.8766483068466187, 0.8963066935539246, 0.8646237254142761, 0.9165715575218201, 0.8629709482192993, 0.877708375453949, 0.8491557240486145, 0.839345395565033, 0.8779978156089783, 0.9068470001220703, 0.9238489270210266, 0.9165697693824768, 0.9035691022872925, 0.8718569278717041, 0.8794474005699158, 0.8158349990844727, 0.8325138688087463, 0.8752139806747437, 0.8602566719055176, 0.8778356313705444, 0.9090604186058044, 0.8783048987388611, 0.849195122718811, 0.819081723690033, 0.884467601776123, 0.9282583594322205, 0.9230634570121765, 0.86381995677948, 0.9144763350486755, 0.9044052362442017, 0.8651371002197266, 0.884066104888916, 0.8797667622566223, 0.8360410332679749, 0.9407731890678406, 0.869777500629425, 0.912273645401001, 0.8807462453842163, 0.9002940654754639, 0.8516364693641663, 0.8919559121131897, 0.8890770673751831, 0.8902016282081604, 0.9184189438819885, 0.8911997675895691, 0.849628210067749, 0.8810040950775146, 0.93601393699646, 0.8946440815925598, 0.8318599462509155, 0.8521263003349304, 0.8742708563804626, 0.8874363899230957, 0.9236658811569214, 0.848228394985199, 0.9025195240974426, 0.9146159291267395, 0.8782015442848206, 0.8972859382629395, 0.8960816860198975, 0.9112850427627563, 0.9186965823173523, 0.8643410801887512, 0.8462550640106201, 0.8601163029670715, 0.874690055847168, 0.8196651935577393, 0.8638023734092712, 0.8731020092964172, 0.8956975340843201, 0.8373807072639465, 0.8818223476409912, 0.903903067111969, 0.8828436136245728, 0.8068558573722839, 0.8541696667671204, 0.8494818210601807, 0.8805930018424988, 0.9138222336769104, 0.8956554532051086, 0.8654539585113525, 0.8366716504096985, 0.8434338569641113, 0.9064345955848694, 0.8734634518623352, 0.8473653197288513, 0.8443239331245422, 0.8520967960357666, 0.9042349457740784, 0.8567454814910889, 0.8609882593154907, 0.9464460611343384, 0.8969486355781555, 0.8367010951042175, 0.8417015671730042, 0.8463584780693054, 0.8315024971961975, 0.8543277978897095, 0.9061422348022461, 0.8444402813911438, 0.8711910247802734, 0.8718900680541992, 0.8694939613342285, 0.8748149871826172, 0.8872876763343811, 0.9191025495529175, 0.9037599563598633, 0.8717758059501648, 0.8573964238166809, 0.8254151344299316, 0.9183419346809387, 0.9125710129737854, 0.9395158290863037, 0.9182981252670288, 0.8372817039489746, 0.8964715003967285, 0.8251665830612183, 0.9247353672981262, 0.8618353605270386, 0.9235032200813293, 0.9078940153121948, 0.8277881741523743, 0.8680815100669861, 0.8578265905380249, 0.8939101099967957, 0.8329551815986633, 0.9096936583518982, 0.8924238681793213, 0.8930985927581787, 0.8663074374198914, 0.9035530686378479, 0.8731077313423157, 0.8630613684654236, 0.8884127736091614, 0.8780468702316284, 0.8862988352775574, 0.9486570954322815, 0.8981525897979736, 0.8622857928276062, 0.8604278564453125, 0.8823725581169128, 0.8296616673469543, 0.8591855764389038, 0.8537747859954834, 0.8606301546096802, 0.892696738243103, 0.9003029465675354, 0.848090648651123, 0.8645774126052856, 0.8743971586227417, 0.8419823050498962, 0.8692058324813843, 0.8995562195777893, 0.8765067458152771, 0.8512138724327087, 0.8968640565872192, 0.8688842058181763, 0.9185741543769836, 0.8473073840141296, 0.889906108379364, 0.8946206569671631, 0.8446953892707825, 0.9125742316246033, 0.9334138631820679, 0.9383562207221985, 0.8448072671890259, 0.8306758403778076, 0.925032377243042, 0.886078953742981, 0.8512817621231079, 0.8698438405990601, 0.8919661045074463, 0.9658950567245483, 0.913176417350769, 0.8685817122459412, 0.9015102386474609, 0.861405074596405, 0.8713558912277222, 0.8829448819160461, 0.8816527724266052, 0.9249058365821838, 0.8347388505935669, 0.9347204566001892, 0.8952317237854004, 0.8752239346504211, 0.9234018325805664, 0.8114303946495056, 0.8788451552391052, 0.9439854025840759, 0.8481572270393372, 0.8975011706352234, 0.9216588139533997, 0.9381494522094727, 0.859588623046875, 0.8496479988098145, 0.9399027824401855, 0.8603484034538269, 0.9770146012306213, 0.909217119216919, 0.8822999000549316, 0.8781647086143494, 0.88901686668396, 0.8767006993293762, 0.8767601847648621, 0.9014393091201782, 0.8480576276779175, 0.8920024037361145, 0.9400672912597656, 0.8674456477165222, 0.9149863719940186, 0.8966991901397705, 0.920413613319397, 0.860267162322998, 0.8644909262657166, 0.8913478255271912, 0.7730042338371277, 0.8713861703872681, 0.8986781239509583, 0.8673933148384094, 0.9575628638267517, 0.8628442287445068, 0.8661938905715942, 0.8647627830505371, 0.8952943086624146, 0.8569360375404358, 0.8913403153419495, 0.8134182095527649, 0.8593413233757019, 0.8673235774040222, 0.8662654757499695, 0.790173351764679, 0.8359297513961792, 0.8630532026290894, 0.8525615930557251, 0.8882034420967102, 0.978289783000946, 0.8567412495613098, 0.9592854976654053, 0.9224409461021423, 0.863551914691925, 0.978922963142395, 0.8721136450767517, 0.8602610230445862, 0.8778560757637024, 0.8838891983032227, 0.8888506293296814, 0.9790470600128174, 0.9043294787406921, 0.8035563826560974, 0.8838856816291809, 0.9230493903160095, 0.9103057980537415, 0.835624098777771, 0.8559724688529968, 0.9121591448783875, 0.9264891147613525, 0.8470790982246399, 0.9356039762496948, 0.8760191798210144, 0.8911389112472534, 0.9656203389167786, 0.8958127498626709, 0.8310064077377319, 0.8991631269454956, 0.8523537516593933, 0.896204948425293, 0.836129367351532, 0.8965063095092773, 0.9078817367553711, 0.8368261456489563, 0.8872658610343933, 0.8820228576660156, 0.8912556171417236, 0.8800363540649414, 0.909959077835083, 0.8446269035339355, 0.8416138887405396, 0.9209378361701965, 0.8576055765151978, 0.889577329158783, 0.8818786144256592, 0.8489181995391846, 0.8767056465148926, 0.9095319509506226, 0.9703547954559326, 0.9094845652580261, 0.8472367525100708, 0.903151273727417, 0.847011923789978, 0.9607983231544495, 0.7919098734855652, 0.8036854863166809, 0.8282053470611572, 0.84567791223526, 0.8766506314277649, 0.9063349962234497, 0.8958112001419067, 0.8565207123756409, 0.8449364304542542, 0.9078306555747986, 0.8553887009620667, 0.8365489840507507, 0.8572772741317749, 0.8138480186462402, 0.8649001121520996, 0.8188009858131409, 0.8070709109306335, 0.9466754794120789, 0.9287453293800354, 0.8939170241355896, 0.964252233505249, 0.8848767876625061, 0.8075003623962402, 0.8486623167991638, 0.8953496813774109, 0.855618417263031, 0.878000020980835, 0.8632392287254333, 0.8828827738761902, 0.9534799456596375, 0.8069050908088684, 0.9537253379821777, 0.889313817024231, 0.9046788811683655, 0.9252169728279114, 0.9716441631317139, 0.8839061260223389, 0.8774024248123169, 0.8506467938423157, 0.8568246364593506, 0.8727521300315857, 0.855589747428894, 0.8603766560554504, 0.9455636739730835, 0.8586896061897278, 0.8442298173904419, 0.8946072459220886, 0.895250141620636, 0.8855951428413391, 0.9021733999252319, 0.94896000623703, 0.9704219698905945, 0.9266721606254578, 0.8620209693908691, 0.8630005717277527, 0.8940281271934509, 0.8805363178253174, 0.8585909605026245, 0.8872402906417847, 0.9196398258209229, 0.9769805073738098, 0.9829717874526978, 0.9023005962371826, 0.9167728424072266, 0.8472718000411987, 0.9088442921638489, 0.8774169087409973, 0.8774994611740112, 0.9243568778038025, 0.8975797295570374, 0.8794254064559937, 0.9164801836013794, 0.8384755849838257, 0.8116263747215271, 0.8922345638275146, 0.8117092251777649, 0.8719363212585449, 0.8467348217964172, 0.8820971250534058, 0.8573035597801208, 0.8739156126976013, 0.8506314158439636, 0.8808069825172424, 0.8733295202255249, 0.8399200439453125, 0.9019092321395874, 0.9024142622947693, 0.938036322593689, 0.8892537355422974, 0.9636667370796204, 0.8184698224067688, 0.8833494782447815, 0.9515495300292969, 0.8547983169555664, 0.8812376260757446, 0.9017770290374756, 0.8740025162696838, 0.8193844556808472, 0.9016774892807007, 0.8765584230422974, 0.8988617062568665, 0.814603865146637, 0.9765685200691223, 0.9066668152809143, 0.9342092275619507, 0.8914186358451843, 0.9335911273956299, 0.9081621170043945, 0.9178408980369568, 0.9770651459693909, 0.9135825037956238, 0.9049714207649231, 0.8947994112968445, 0.9071605801582336, 0.915766179561615, 0.8740125894546509, 0.8670122623443604, 0.8555828332901001, 0.8873550891876221, 0.9025158286094666, 0.8510839343070984, 0.8892152905464172, 0.8751933574676514, 0.9445446133613586, 0.8829495906829834, 0.8472952246665955, 0.8233348727226257, 0.9307762980461121, 0.8799103498458862, 0.8524919152259827, 0.8942006230354309, 0.8471313714981079, 0.8704289793968201, 0.85563063621521, 0.8345810174942017, 0.8071072697639465, 0.8839871883392334, 0.8461641073226929, 0.8532949090003967, 0.8589417934417725, 0.8466302156448364, 0.848785936832428, 0.8590616583824158, 0.8298903107643127, 0.8226592540740967, 0.9590190052986145, 0.900687575340271, 0.825283944606781, 0.8689712285995483, 0.8745548725128174, 0.9114080667495728, 0.8750271201133728, 0.8831496834754944, 0.8566052913665771, 0.878460168838501, 0.9269757866859436, 0.9234596490859985, 0.9232600927352905, 0.9741936326026917, 0.8581474423408508, 0.8859238624572754, 0.8760457038879395, 0.8631620407104492, 0.9304941296577454, 0.9041690826416016, 0.8373993635177612, 0.8876065611839294, 0.9084769487380981, 0.8908606767654419, 0.8951281309127808, 0.8759205341339111, 0.8600319623947144, 0.8711790442466736, 0.8629053831100464, 0.9453588724136353, 0.8826421499252319, 0.8781930804252625, 0.9583197236061096, 0.9042892456054688, 0.8492525219917297, 0.8992874026298523, 0.8903799653053284, 0.8626673817634583, 0.8447951078414917, 0.9101594090461731, 0.8624184131622314, 0.8795348405838013, 0.8563643097877502, 0.8455021381378174, 0.8737566471099854, 0.8832243084907532, 0.8551089763641357, 0.8565897345542908, 0.9176075458526611, 0.8942348957061768, 0.8857252597808838, 0.8818959593772888, 0.8515657186508179, 0.9230776429176331, 0.8964282870292664, 0.8429079055786133, 0.9133849143981934, 0.8854964375495911, 0.8763867020606995, 0.8355029821395874, 0.8380352258682251, 0.846459150314331, 0.8902517557144165, 0.8919110894203186, 0.813724935054779, 0.9075563549995422, 0.8996919393539429, 0.8808737397193909, 0.8058538436889648, 0.907093346118927, 0.8726111650466919, 0.918929398059845, 0.8787187337875366, 0.8365407586097717, 0.8075402975082397, 0.8851953744888306, 0.8215534687042236, 0.8889344930648804, 0.8118488192558289, 0.8382053971290588, 0.9535444974899292, 0.9498124122619629, 0.8364834189414978, 0.8653895854949951, 0.8764192461967468, 0.8771210312843323, 0.9291895031929016, 0.8799259662628174, 0.9333131909370422, 0.8655996322631836, 0.8981713652610779, 0.9098270535469055, 0.9117745161056519, 0.8810783624649048, 0.8688441514968872, 0.8446220755577087, 0.8464059829711914, 0.8974730372428894, 0.8852242231369019, 0.8323971629142761, 0.876224935054779, 0.8745447993278503, 0.8152493238449097, 0.8428149223327637, 0.8727245330810547, 0.8468920588493347, 0.8511199355125427, 0.8375348448753357, 0.8661165833473206, 0.8926683664321899, 0.8380076289176941, 0.8462390899658203, 0.8710850477218628, 0.9236505627632141, 0.9165957570075989, 0.8594826459884644, 0.8891053199768066, 0.9063178896903992, 0.869939386844635, 0.8283203840255737, 0.8890442848205566, 0.8777991533279419, 0.8743748068809509, 0.8857315182685852, 0.8642199039459229, 0.9505700469017029, 0.8697224855422974, 0.9281408786773682, 0.9694163203239441, 0.8787834048271179, 0.8920336961746216, 0.8810197114944458, 0.8935680389404297, 0.8559041619300842, 0.870002806186676, 0.8235389590263367, 0.8642321228981018, 0.9016436338424683, 0.912280797958374, 0.8533810973167419, 0.8439581990242004, 0.9196357727050781, 0.8557566404342651, 0.8426565527915955, 0.89061439037323, 0.9169278144836426, 0.8421663045883179, 0.8599759936332703, 0.9251308441162109, 0.909614086151123, 0.9762632250785828, 0.8829222321510315, 0.8391322493553162, 0.8594564199447632, 0.8754523396492004, 0.8971431255340576, 0.9216794371604919, 0.8724843859672546, 0.920628547668457, 0.8475831151008606, 0.8625795245170593, 0.8334876298904419, 0.8574077486991882, 0.9191367626190186, 0.9008748531341553, 0.9176778197288513, 0.824135959148407, 0.8719703555107117, 0.9354540109634399, 0.8495060801506042, 0.8865193128585815, 0.864938497543335, 0.8610782027244568, 0.925075888633728, 0.927609384059906, 0.8951232433319092, 0.9094264507293701, 0.8899357914924622, 0.9197337031364441, 0.8399038910865784, 0.8212929368019104, 0.9157910943031311, 0.8664339780807495, 0.9163146615028381, 0.8903637528419495, 0.9038758873939514, 0.8305495977401733, 0.8826047778129578, 0.9176774621009827, 0.981614887714386, 0.8716700077056885, 0.919738233089447, 0.9132850170135498, 0.842416524887085, 0.8485606908798218, 0.8109404444694519, 0.8602622151374817, 0.9320590496063232, 0.8675844073295593, 0.8687832951545715, 0.919590950012207, 0.8755570650100708, 0.8890165090560913, 0.8903385996818542, 0.8998174071311951, 0.8420563340187073, 0.8607121109962463, 0.8283466100692749, 0.9329432249069214, 0.8332376480102539, 0.866867184638977, 0.857514500617981, 0.8179767727851868, 0.9076298475265503, 0.8644972443580627, 0.8516683578491211, 0.9068843126296997, 0.8524098992347717, 0.8744572997093201, 0.7774330377578735, 0.9383518099784851, 0.879784107208252, 0.8862442970275879, 0.8470070362091064, 0.8942362666130066, 0.8180409073829651, 0.8820109367370605, 0.8829057812690735, 0.8871652483940125, 0.8627743721008301, 0.8425596356391907, 0.8650059700012207, 0.8836302757263184, 0.9020735025405884, 0.906206488609314, 0.9131420850753784, 0.8694238662719727, 0.8876233696937561, 0.890145480632782, 0.8981097340583801, 0.8389931321144104, 0.898444652557373, 0.8978495001792908, 0.8606042265892029, 0.8715357184410095, 0.9259623289108276, 0.8469748497009277, 0.8504714965820312, 0.9177951216697693, 0.9528236985206604, 0.8966037034988403, 0.891089677810669, 0.9075919389724731, 0.9186098575592041, 0.8717904090881348, 0.8468583822250366, 0.8680087924003601, 0.8818292021751404, 0.9001086950302124, 0.840785026550293, 0.8377528190612793, 0.9076644778251648, 0.8523784875869751, 0.892662763595581, 0.8718674182891846, 0.8777958750724792, 0.8702239394187927, 0.8695023059844971, 0.870621919631958, 0.8736842274665833, 0.8575920462608337, 0.938652753829956, 0.9811455607414246, 0.86005699634552, 0.8289486169815063, 0.8731843829154968, 0.881188154220581, 0.9004552364349365, 0.8807687163352966, 0.8870561718940735, 0.8720284104347229, 0.903594434261322, 0.90267413854599, 0.841886043548584, 0.8324567675590515, 0.923380970954895, 0.90339195728302, 0.9173129796981812, 0.8628248572349548, 0.8547749519348145, 0.8693280816078186, 0.9233769774436951, 0.9190338850021362, 0.8442292809486389, 0.8711657524108887, 0.8380042314529419, 0.9294734597206116, 0.9141945242881775, 0.9127090573310852, 0.9580410122871399, 0.9030840396881104, 0.875740647315979, 0.9179303646087646, 0.8880013227462769, 0.8824815154075623, 0.8500153422355652, 0.8599706292152405, 0.8799059987068176, 0.8344773054122925, 0.9105877876281738, 0.954444944858551, 0.8927087783813477, 0.8740521669387817, 0.9293665289878845, 0.8678174018859863, 0.8606929779052734, 0.9553697109222412, 0.905604898929596, 0.8993674516677856, 0.8818041086196899, 0.9175188541412354, 0.9248976707458496, 0.8590537905693054, 0.8520147204399109, 0.9186272621154785, 0.8395159244537354, 0.8580998778343201, 0.8663779497146606, 0.8792716860771179, 0.8811618685722351, 0.8184444308280945, 0.8240089416503906, 0.9256675243377686, 0.9288428425788879, 0.8579345345497131, 0.8906124234199524, 0.8246186375617981, 0.9317114353179932, 0.9096004366874695, 0.8545126914978027, 0.8357528448104858, 0.8597428202629089, 0.901453971862793, 0.8869432210922241, 0.843934953212738, 0.8338809609413147, 0.8681419491767883, 0.8744882941246033, 0.8677812814712524, 0.8471142649650574, 0.827792227268219, 0.9000523090362549, 0.8151331543922424, 0.841679573059082, 0.8903578519821167, 0.8472310304641724, 0.8455728888511658, 0.8909181356430054, 0.899955153465271]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test BLEU-1: {bleu['precisions'][0]}\")\n",
    "print(f\"Test BLEU-2: {bleu['precisions'][1]}\")\n",
    "print(f\"Test BLEU-3: {bleu['precisions'][2]}\")\n",
    "print(f\"Test BLEU-4: {bleu['precisions'][3]}\")\n",
    "print(f\"Test METEOR: {meteor['meteor']}\")\n",
    "print(f\"Test BERTScore: {bertscore['f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-31T16:51:38.391828Z",
     "iopub.status.busy": "2024-03-31T16:51:38.391416Z",
     "iopub.status.idle": "2024-03-31T16:51:38.437877Z",
     "shell.execute_reply": "2024-03-31T16:51:38.436608Z",
     "shell.execute_reply.started": "2024-03-31T16:51:38.391797Z"
    }
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T16:58:28.485259Z",
     "iopub.status.busy": "2024-03-31T16:58:28.484869Z",
     "iopub.status.idle": "2024-03-31T16:58:46.368108Z",
     "shell.execute_reply": "2024-03-31T16:58:46.366632Z",
     "shell.execute_reply.started": "2024-03-31T16:58:28.485223Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T16:58:46.370821Z",
     "iopub.status.busy": "2024-03-31T16:58:46.369936Z",
     "iopub.status.idle": "2024-03-31T16:58:54.354623Z",
     "shell.execute_reply": "2024-03-31T16:58:54.353347Z",
     "shell.execute_reply.started": "2024-03-31T16:58:46.370781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /raid/home/akshat21515/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /raid/home/akshat21515/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /raid/home/akshat21515/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load('bleu')\n",
    "meteor_metric = evaluate.load('meteor')\n",
    "bertscore_metric = evaluate.load('bertscore')\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer_2c = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_2c = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T16:58:54.356852Z",
     "iopub.status.busy": "2024-03-31T16:58:54.356447Z",
     "iopub.status.idle": "2024-03-31T16:58:54.363000Z",
     "shell.execute_reply": "2024-03-31T16:58:54.361962Z",
     "shell.execute_reply.started": "2024-03-31T16:58:54.356821Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    example['de'] = tokenizer_2c(example['de'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    example['en'] = tokenizer_2c(example['en'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T16:58:54.364863Z",
     "iopub.status.busy": "2024-03-31T16:58:54.364524Z",
     "iopub.status.idle": "2024-03-31T17:00:00.043907Z",
     "shell.execute_reply": "2024-03-31T17:00:00.042368Z",
     "shell.execute_reply.started": "2024-03-31T16:58:54.364834Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 50000/50000 [00:26<00:00, 1858.80 examples/s]\n",
      "Map: 100%|| 2169/2169 [00:01<00:00, 1948.97 examples/s]\n",
      "Map: 100%|| 2999/2999 [00:01<00:00, 1737.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_input = Dataset.from_list(train_data['translation']).map(tokenize_function)\n",
    "model_val = Dataset.from_list(val_data['translation']).map(tokenize_function)\n",
    "model_test = Dataset.from_list(test_data['translation']).map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T17:00:00.049118Z",
     "iopub.status.busy": "2024-03-31T17:00:00.048575Z",
     "iopub.status.idle": "2024-03-31T17:00:00.058241Z",
     "shell.execute_reply": "2024-03-31T17:00:00.056541Z",
     "shell.execute_reply.started": "2024-03-31T17:00:00.049072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define translation function\n",
    "def translate_2C(model,tokenizer,text:str):\n",
    "    input_text = \"translate German to English: \" + text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(input_ids)\n",
    "    en_translation = tokenizer.decode(outputs[0])\n",
    "\n",
    "    return en_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T17:00:00.060135Z",
     "iopub.status.busy": "2024-03-31T17:00:00.059769Z",
     "iopub.status.idle": "2024-03-31T17:00:00.093086Z",
     "shell.execute_reply": "2024-03-31T17:00:00.092032Z",
     "shell.execute_reply.started": "2024-03-31T17:00:00.060104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate_2c(model,tokenizer,data):\n",
    "    translations = []\n",
    "    references = []\n",
    "    for item in data:\n",
    "        de_text = item['translation']['de']\n",
    "        en_translation = translate_2C(model,tokenizer,de_text)\n",
    "        translations.append(en_translation)\n",
    "        references.append(item['translation']['en'])\n",
    "\n",
    "    bleu = bleu_metric.compute(predictions=translations, references=references)\n",
    "    meteor = meteor_metric.compute(predictions=translations, references=references)\n",
    "    # bertscore = bertscore_metric.compute(predictions=translations, references=references, model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "    return bleu, meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T17:01:11.090992Z",
     "iopub.status.busy": "2024-03-31T17:01:11.089250Z",
     "iopub.status.idle": "2024-03-31T17:01:53.975036Z",
     "shell.execute_reply": "2024-03-31T17:01:53.972944Z",
     "shell.execute_reply.started": "2024-03-31T17:01:11.090865Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;43;03m#     fp16=True\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Define trainer\u001b[39;00m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_2c,\n\u001b[1;32m     19\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_2c,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mevaluate\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m<string>:130\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/training_args.py:1605\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m   1600\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1606\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1607\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1609\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1611\u001b[0m ):\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1614\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or MLU devices or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1615\u001b[0m     )\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1627\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1628\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/training_args.py:2094\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2094\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/generic.py:63\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     61\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/training_args.py:2000\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   1999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2000\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2001\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2002\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2003\u001b[0m         )\n\u001b[1;32m   2004\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "#     fp16=True\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_2c,\n",
    "    tokenizer=tokenizer_2c,\n",
    "    args=training_args,\n",
    "    train_dataset=model_input,\n",
    "    eval_dataset=model_val,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer_2c, model=model_2c, max_length=128),\n",
    "    compute_metrics=evaluate\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "trainer.save_model('fine_tuned_t5-small')\n",
    "\n",
    "train_loss = trainer.state.log_history[\"loss\"]\n",
    "val_loss = trainer.state.log_history[\"eval_loss\"]\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T17:02:05.998057Z",
     "iopub.status.busy": "2024-03-31T17:02:05.997628Z",
     "iopub.status.idle": "2024-03-31T17:11:53.154735Z",
     "shell.execute_reply": "2024-03-31T17:11:53.152362Z",
     "shell.execute_reply.started": "2024-03-31T17:02:05.998017Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate on validation data after training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m val_bleu, val_meteor, val_bertscore \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU-1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_bleu[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecisions\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m      6\u001b[0m     de_text \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m     en_translation \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_2C\u001b[49m\u001b[43m(\u001b[49m\u001b[43mde_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     translations\u001b[38;5;241m.\u001b[39mappend(en_translation)\n\u001b[1;32m      9\u001b[0m     references\u001b[38;5;241m.\u001b[39mappend(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mtranslate_2C\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslate German to English: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text\n\u001b[1;32m      4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m en_translation \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m en_translation\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1510\u001b[0m         input_ids,\n\u001b[1;32m   1511\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1524\u001b[0m     )\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2411\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2408\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2411\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1748\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1745\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1748\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1115\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1101\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1102\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         output_attentions,\n\u001b[1;32m   1113\u001b[0m     )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:755\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    752\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:344\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    343\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 344\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:298\u001b[0m, in \u001b[0;36mT5DenseActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8\n\u001b[1;32m    296\u001b[0m ):\n\u001b[1;32m    297\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 298\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate on validation data after training\n",
    "val_bleu, val_meteor = evaluate_2c(model_2c,tokenizer_2c,val_data)\n",
    "print(\"Validation Metrics:\")\n",
    "print(f\"BLEU-1: {val_bleu['precisions'][0]}\")\n",
    "print(f\"BLEU-2: {val_bleu['precisions'][1]}\")\n",
    "print(f\"BLEU-3: {val_bleu['precisions'][2]}\")\n",
    "print(f\"BLEU-4: {val_bleu['precisions'][3]}\")\n",
    "print(f\"Meteor Score: {val_meteor['score']}\")\n",
    "# print(f\"Validation BERTScore: {val_bertscore['score']}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_bleu, test_meteor = evaluate_2c(model_2c,tokenizer_2c,test_data)\n",
    "print(\"Test Metrics:\")\n",
    "print(f\"BLEU-1: {test_bleu['precisions'][0]}\")\n",
    "print(f\"BLEU-2: {test_bleu['precisions'][1]}\")\n",
    "print(f\"BLEU-3: {test_bleu['precisions'][2]}\")\n",
    "print(f\"BLEU-4: {test_bleu['precisions'][3]}\")\n",
    "print(f\"Test METEOR: {test_meteor['meteor']}\")\n",
    "# print(f\"Test BERTScore: {sum(test_bertscore['precision'])/len(test_bertscore['precision'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence:str):\n",
    "    print(\"Original Sentence: \", sentence)\n",
    "    print(f\"2A: {translate_2A(translator, tokenizer_de, tokenizer_en, sentence)}\")\n",
    "    print(f\"2B: {translate_2B(model_2b, tokenizer, sentence)}\")\n",
    "    print(f\"2C: {translate_2C(model_2c, tokenizer, sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def translate_csv(csv_file:str):\n",
    "    de_sents = []\n",
    "    en_sents = []\n",
    "    df = pd.read_csv(csv_file)\n",
    "    for sentence in df['de']:\n",
    "        en_translation = translate_sentence(sentence)\n",
    "        de_sents.append(sentence)\n",
    "        en_sents.append(en_translation)\n",
    "\n",
    "    translated_df = pd.DataFrame({'de':de_sents, 'en':en_sents})\n",
    "    translated_df.to_csv('translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  Mal sehen, wie gut Sie diese Aufgabe gemeistert haben.\n",
      "2A: I would like to be the European European European European European European European European European European\n",
      "2B: <pad> Leider hat man es nicht geschafft, die ganze Aufgabe zu meister\n",
      "2C: <pad> Leider hat man es nicht geschafft, die ganze Aufgabe zu meister\n"
     ]
    }
   ],
   "source": [
    "translate_sentence(\"Mal sehen, wie gut Sie diese Aufgabe gemeistert haben.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
