{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NLP Assignment 1\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1h-k-D1ZXZkk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vb6W49OSb6Rv"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self):\n",
        "    self.vocab = defaultdict(int)\n",
        "    self.all_tokens= defaultdict(int)  # all tokens from first to last\n",
        "    self.merge_rules=[]\n",
        "    self.final_tokens=[]         # only the final tokens\n",
        "\n",
        "\n",
        "\n",
        "  def merge_vocabulary(self,pair, vocab_old):\n",
        "    vocab_new = {}\n",
        "    self.merge_rules.append(pair)\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    regex=re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "\n",
        "   # merge the old vocab based on the new merging rule\n",
        "    for word in vocab_old:\n",
        "        w_out = regex.sub(''.join(pair), word) #find all words where bigram was found in the old vocab\n",
        "        all_tokens=w_out.split()\n",
        "        for a in all_tokens: self.all_tokens[a]+=1\n",
        "        vocab_new[w_out] = vocab_old[word]\n",
        "    return vocab_new\n",
        "\n",
        "\n",
        "\n",
        "  def learn_vocabulary(self, corpus, num_merges):\n",
        "      ### create a list of strings\n",
        "      data = corpus.split('\\n')\n",
        "\n",
        "      ### updating dict vocab with words and frequencies\n",
        "      for line in data:\n",
        "          for word in line.split():\n",
        "            new_word=' '.join(list(word)) + ' $'\n",
        "            self.vocab[new_word] += 1\n",
        "\n",
        "            all_tokens=new_word.split()\n",
        "            for a in all_tokens: self.all_tokens[a]+=1\n",
        "\n",
        "\n",
        "      ### making pairs and updating their frequencies\n",
        "      for _ in range(num_merges):\n",
        "          pairs = defaultdict(int)\n",
        "          for word,freq in self.vocab.items():\n",
        "              chars = word.split()\n",
        "              for j in range(len(chars)-1):\n",
        "                  pairs[chars[j],chars[j+1]] += freq\n",
        "\n",
        "\n",
        "          best_pair = max(pairs, key=pairs.get)\n",
        "          self.vocab = self.merge_vocabulary(best_pair, self.vocab)\n",
        "\n",
        "      t = \" \".join(list(self.vocab.keys()))\n",
        "      d=defaultdict(int)\n",
        "      l=t.split()\n",
        "      for a in l: d[a]+=1\n",
        "      self.final_tokens=list(d.keys())\n",
        "\n",
        "\n",
        "\n",
        "  def tokenize(self,text_lists):\n",
        "    # divide the text into individual letters\n",
        "\n",
        "     ans_list=[]\n",
        "     for a in text_lists:\n",
        "          text=a\n",
        "          data = text.split('\\n')\n",
        "          new_text=\"\"\n",
        "          for line in data:\n",
        "              for word in line.split():\n",
        "                  new_word=' '.join(list(word)) + ' $ '\n",
        "                  new_text+=new_word\n",
        "\n",
        "          # Tokenize the text based on merge rules\n",
        "          merge_rules=self.merge_rules\n",
        "          for rule in merge_rules:\n",
        "              merged_token = \"\".join(rule)\n",
        "              new_text = new_text.replace(\" \".join(rule),merged_token)\n",
        "\n",
        "          tokens = new_text.split()\n",
        "          # print(tokens)\n",
        "          # print()\n",
        "          ans_list.append(tokens)\n",
        "\n",
        "\n",
        "     return ans_list\n",
        "  \n",
        "\n",
        "  def write_to_file(self,root,text_list):\n",
        "      token_path = os.path.join(root,\"tokens.txt\")\n",
        "      rules_path = os.path.join(root,\"merge_rules.txt\")\n",
        "      samples_path = os.path.join(root,\"tokenized_samples.txt\")\n",
        "\n",
        "      file=open(token_path,\"w+\")\n",
        "      for a in list(self.all_tokens.keys()):\n",
        "          file.write(a+\"\\n\")\n",
        "      file.close()\n",
        "\n",
        "\n",
        "      file=open(rules_path,\"w+\")\n",
        "      for a in self.merge_rules:\n",
        "          file.write(a[0]+\",\"+a[1]+\"\\n\")\n",
        "      file.close()\n",
        "\n",
        "\n",
        "      file=open(samples_path,\"w\")\n",
        "      for a in text_list:\n",
        "          s=\",\".join(a)+\"\\n\"\n",
        "          print(s)\n",
        "          file.write(s)\n",
        "      file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIchd84qlplY",
        "outputId": "a2bc1cc2-d4eb-47c1-c810-b4e37c5167f2"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    num_merges = 500\n",
        "\n",
        "    file=open(\"./corpus/corpus.txt\",\"r\")\n",
        "    corpus=file.read()\n",
        "    # file.seek(0)\n",
        "    # test_corpus=file.readlines()\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.learn_vocabulary(corpus, num_merges)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T,o,ken,i,z,ation$,is$,the$,proc,ess$,of$,brea,king$,down$,a$,se,qu,ence$,of$,tex,t$,into$,s,maller$,un,its$,called$,to,ken,s,,,$,which$,can$,be$,wor,d,s,,,$,p,h,rases,,,$,or$,even$,in,di,vi,du,al$,charac,ter,s,.,$\n",
            "\n",
            "T,o,ken,i,z,ation$,is$,of,ten$,the$,fir,st$,step$,in$,n,atural$,l,angu,ages$,proc,es,sing$,tas,ks$,su,ch$,as$,tex,t$,cl,as,si,fication,,,$,n,amed$,en,tity$,re,co,g,n,ition,,,$,and$,sen,timent$,an,al,y,sis,.,$\n",
            "\n",
            "T,he$,resul,ting$,to,ken,s$,are$,t,y,p,ically$,used$,as$,in,pu,t$,to$,fur,ther$,proc,es,sing$,step,s,,,$,su,ch$,as$,v,e,c,tori,z,ation,,,$,where$,the$,to,ken,s$,are$,con,ver,ted$,into$,n,u,merical$,represen,tations$,for$,machine$,lear,ning$,mo,del,s$,to$,use,.,$\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    test_corpus=['''Tokenization is the process of breaking down a sequence of text into smaller units called tokens, which can be words, phrases, or even individual characters.''',\n",
        "    '''Tokenization is often the first step in natural languages processing tasks such as text classification, named entity recognition, and sentiment analysis.''',\n",
        "    '''The resulting tokens are typically used as input to further processing steps, such as vectorization, where the tokens are converted into numerical representations for machine learning models to use.'''  ]\n",
        "    \n",
        "    text_list=tokenizer.tokenize(test_corpus)\n",
        "\n",
        "    tokenizer.write_to_file(\"./merged_tokens/\",text_list)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
